{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayfred/MissionR-D/blob/main/SAFE_application.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRP7dufeAThK",
        "outputId": "90eb5c6b-264f-4f64-f4f3-0f631a0b12ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/MyDrive/Data/onlyEnComments.csv\"\n",
        "df_content = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIoLc4EtUxYi"
      },
      "source": [
        "Chargement des données CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IsOHf05WUw0U"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# open the CSV file using the csv module\n",
        "with open(path, newline='',  encoding='utf-8') as csvfile:\n",
        "    # create a csv reader object\n",
        "    reader = csv.reader(csvfile)\n",
        "    # create an empty list to store the strings\n",
        "    string_list = []\n",
        "    # loop through each row in the CSV file\n",
        "    for row in reader:\n",
        "        # add the string to the list\n",
        "        #if detect(row[0]) == 'fr':\n",
        "        string_list.append(row[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "string_list = string_list[:50]"
      ],
      "metadata": {
        "id": "VrTWfKl3FgSS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRWZ_PLpZCyr",
        "outputId": "63ca8afc-b61f-4787-8ed7-e4be517d0024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good but buggy. There also appears to be no way to report bugs. Update: Still 2 stars. Yes, you can contact them (via chat or call), but my experience is they tell you to reset the watch, which, in my experience, helps nothing, and requires you to set your watch back up (manually), because \"backup\" doesn't back up everything. Maybe the other watches aren't any better.\n"
          ]
        }
      ],
      "source": [
        "print((string_list[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3GNJuYWTitA"
      },
      "source": [
        "Pre-traitement du texte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AucR-PxMW9Mf",
        "outputId": "0485f697-987a-45b4-d812-231a2f1bbc44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-04-11 07:46:20.077072: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9IAx-LTiXIyb"
      },
      "outputs": [],
      "source": [
        "#Etapes du texte Preprocessing\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "listStopWords = [str(x) for x in nlp.Defaults.stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "K0s-fFW2TgdN"
      },
      "outputs": [],
      "source": [
        "def united(text):\n",
        "    doc =  text.split(\" \");\n",
        "    # Itérer à travers chaque token dans le document\n",
        "    for token in doc:\n",
        "        if token in [\"because\", \"since\", \"as\", ]:\n",
        "            doc = doc[:doc.index(token)]\n",
        "            break\n",
        "        #Remove explanations\n",
        "        # Si le token est \"parce que\", \"car\" ou \"puisque\", supprimer tous les tokens suivants dans la phrase\n",
        "        \n",
        "    \n",
        "    # Supprimer tout ce qui est entre parenthèses\n",
        "    sentence = ' '.join(doc)\n",
        "    sentence = re.sub(r'\\([^()]*\\)', '', sentence)\n",
        "\n",
        "    #print(f\"remove explanations : {sentence}\")\n",
        "\n",
        "    sentence = re.sub('http[s]?://\\S+', '', sentence)\n",
        "\n",
        "    #print(f\"remove links: {sentence}\")\n",
        "\n",
        "\n",
        "    # Regular expressions for phone numbers and email addresses\n",
        "    quotes = r'\\\"[^\\\"]+\\\"'\n",
        "    phone_regex = r\"(?<!\\d)(?:\\d[ -/\\\\_\\d]*){10}(?!\\d)\"\n",
        "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"  # matches john.doe@example.com\n",
        "\n",
        "    # Find phone numbers and email addresses in the sentence\n",
        "    sentence = re.sub(quotes,\"\", sentence)\n",
        "\n",
        "    #print(f\"remove links2: {sentence}\")\n",
        "\n",
        "\n",
        "    sentence = re.sub(phone_regex,\"\", sentence)\n",
        "    sentence = re.sub(email_regex,\"\", sentence)\n",
        "    sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
        "    doc = sentence.split(\" \")\n",
        "    for word in doc:\n",
        "        if word in listStopWords:\n",
        "            doc.remove(word)\n",
        "    \n",
        "    sentence = ' '.join(doc)\n",
        "\n",
        "    #print(f\"remove phone numbers and contacts: {sentence}\")\n",
        "\n",
        "    #Removal of ' and the char that precede it\n",
        "    sentence = sentence.replace(\"'\", \" \")\n",
        "\n",
        "    #print(f\"remove leopaul: {sentence}\")\n",
        "\n",
        "    words = sentence.split(\" \")\n",
        "    newSentence = \"\"\n",
        "    for word in words:\n",
        "        if len(word) > 1:\n",
        "            newSentence += word + \" \"\n",
        "    sentence = newSentence\n",
        "    \n",
        "    #print(f\"remove quotes : {sentence}\")\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "    #texte = doc.text\n",
        "    text = \"\"\n",
        "    for token in doc:\n",
        "        if (token.pos_ == \"PRON\" or token.pos_ == \"DET\"):\n",
        "            continue\n",
        "        else:\n",
        "            if not token.text.__contains__(\"'\"):\n",
        "                text += token.text\n",
        "                text += \" \"\n",
        "    sentence = text\n",
        "\n",
        "    #print(f\"remove pronoum and determinant : {sentence}\")\n",
        "\n",
        "\n",
        "    doc = nlp(sentence)\n",
        "    result = []\n",
        "    for token in doc:\n",
        "        if not (token.ent_type_ == \"ORG\" or token.ent_type_ == \"PERSON\" or token.text.istitle()):\n",
        "            result.append(token.text)\n",
        "    sentence =  \" \".join(result)\n",
        "    #print(f\"remove proper noun : {sentence}\")\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\"\"\"\n",
        "for string in string_list:\n",
        "    united_string = united(string)\n",
        "    string_list_clean.append(united_string)\n",
        "print(string_list_clean)\n",
        "#run\n",
        "#print(united(text))\"\"\"\n",
        "united_string = []\n",
        "for i in range(len(string_list)):\n",
        "  united_string += [united(string_list[i])]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7ppmsgWCiQXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b86aaf0-d0df-4648-e97c-82d93b9934f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['designed good selection workout options does good job tracking sleep wish option add note journal entry sleep session if was simple regards watch app think should more breathing cycles section',\n",
              " 'buggy appears be way report bugs stars can contact experience tell to reset watch experience helps requires to set watch up',\n",
              " 'best frequently don work composition works maybe 30 time then stops watch isn tight arms aren far out maybe',\n",
              " 'about',\n",
              " 'synced app used have problems capability frequently experience workouts are synced app restarted sync worked fine couple days doesn AGAIN VERY frustrating distance not close being accurate jnside apartment building when walking halls near parameter building',\n",
              " 'app slowly losing features long time has been chore but simply tedious example to scan food labels anymore be manually entered doesn care app',\n",
              " 'good app be improved has downgraded little ways instance scheduling workouts be customizable have daily weekly routine set by user not be days automatically now problem have download workout vids time use program is kinda dumb waste time overall good meassure steps distance',\n",
              " 'fine phone just stopped records 10 steps found by think sleep monitoring very useful can not manually enter data back sleep get up bathroom then back bed way account that app not allow interrupted sleep enter start and end else',\n",
              " 'to decent job collecting data watch sleep data messed sit read book kitchen living room records',\n",
              " 'app very good sleep party amazing step count very inaccurate on long gps worthy walk neighborhood great inside house walking figure how turn challenge started is great',\n",
              " 'use app everyday record daily walk when completed check distance time step count best feature fact review route map feature worked up walk went review route feature now removed really disappointing app return map route feature',\n",
              " 'time update gets worse longer workout notifications steps longer unless into app weekly monthly bar graphs vanished favor stop updating and way was give zero stars soon',\n",
              " 'app functionality broken past week use galaxy watch track of workouts won load calendar longer review workouts used allow track workouts watch left home forgot charge no longer available reduce functionality needed add functionality like adding custome exercises to watch',\n",
              " 'checked was suggested turned notifications steps back again restarted phone fixed issue have screenshot show looks like typically like app update now steps not showing on main screen ve gone settings though didn change',\n",
              " 'useful works trouble sleeping wanted monitor sleep tracker works 20 time thought watch issue retired one wearing upgraded changes ve uninstalled reinstalled app multiple times cleared cache and rebooted watch factory reset watch reset phone loss want work like supposed to',\n",
              " 'app have',\n",
              " 'recently upgraded watch can super pleased finding absolutely doesn track floors properly after ve tricks suggested thinks sleeping watching movie add sleep time can edit also impossible edit deleting entire sleep night watch changed screens times prompting also start unknown exercises prompting bonus get exercise options',\n",
              " 'calories burned removed now can see calories burned exercising previous days makes difficult calorie deficit focused support told clear cliche data turn phone for minute watch monitoring heart properly would nice to see estimated total calories burned day is other apps benefit',\n",
              " 'charts use work combine sleep there multiple segments design only shows last day makes useless for trying compare trends sleep cycles responded clearly didn read feedback design error isn going to parrot nonsense troubleshooting steps',\n",
              " 'stars',\n",
              " 'wouldn let sign with account keeps saying lie takes screen screen saying allow certain things use app need record data watch is connected to phone need app to track steps that watch ffs step keeps getting worse downloading',\n",
              " 'wake middle night either read morning studying watch movie look phone shows sleeping wish edit time sleep record like on edit option thing re allowed do delete entire record of sleeping night',\n",
              " 'menu edit sleep lets delete watching movie detects',\n",
              " 'nights didn record exercises did record heart rate sync with VR app app recorded exercise fine app recorded heart rate 85 showed in 150 didn record exercise randomly recording heart rate hours time yes watch on correctly ve rebooted watch phone is really frustrating',\n",
              " 've liked app in last month reason lost ability track speed steps distance treadmill one things use ve do sure would work ve tried figure where send screen shots can not figure out watch works fine reset day work ve reset times thing happens',\n",
              " 'like app wondering cup went 8 oz 4 oz tracked logged water intake food log only allows entries be nice option add additional meals times be big help app scan bar code pick nutritional information item search create time consuming',\n",
              " 'rate recovery data gone today workout always shown heart rate zones not appear app great good news called number provided able speak representative minute great news bad news is known recent defect that working sure couldn answered question directly than telling to',\n",
              " 'last weeks steps counter wildly inaccurate app says up night long walking asleep not wear watch night phone stays plugged next bed times doesn count steps up walking says walking not sure changed app become useless',\n",
              " 'app great is missing things way basics levels riding bicycle ability connect cadence sensor smartwatch wanna next level incorporate glucose sensor at activate one device would awesome is killing with apple watch pro need to represent gon na replaced',\n",
              " 'WHY include time total sleep time graph opposite asleep service knowledgeable didn synchronize update watch turned didn suggest check told couldn airplane mode is false fine airplane mode use cellular',\n",
              " 'average speeds reversed impossible average speed be higher maximum speed simple math calculation off around 20 compared multiple sources app fairly easy use wife find useful monitoring relative progress vs real numbers rate obvious fixes done',\n",
              " 'functions don work old phone monitor heart rate stress level oxygen etc upgraded that of app works installed and reinstalled several times great app older model phones guess have find one',\n",
              " 'pretty good app there things use instance able save workout program later use do would like try to searching thing app separate just',\n",
              " 'best app fitness hands free easy customize meals adjust servings even add new food gives science based information new features added time needs improvements tracking vitamins impossible app food 60 time incorrect calories missing nutrient info needs add ability scan smart label import fitness app there hands',\n",
              " 'main function app weight loss management removed of features still can manually set calorie target weight target no longer integrated liked having calorie target automatically adjust based weight loss goals',\n",
              " 'very inconsistent records sleep exercise accurately oftentimes missing very inaccurate is synced watch is giving incomplete data to work fairly no longer primary reason getting phone watch gotten worse',\n",
              " 'wish give like stars problem is beginner ve found tool be extremely helpful wish could tad bit things like see change frequently messages suggest workouts runs based goals experience level change coach voice',\n",
              " 'works fine gosh improve fitness tab barely programs require equipment claim be beginner level particularly fencing program NOT beginners been point do with ease one far beginner know bad could hurt hip trying do 2nd 3rd modified lunges proper stretching practice idea labeling add new beginner courses',\n",
              " 'having major time issues right especially apparent sleep data changed sleep wake time workout times like hours unacceptable ve contacted customer service app said issue be fixed update gave app chance issue to been solved data recorded issue active still corrupted',\n",
              " 'lightweight comprehensive app easy like forgets input data still decent job tracking activity useful features been automatic activity tracker sleep reminder automatic sleep detection seem like gets little worse features decrease update example decision remove stress tracking heart rate tracking features silly inconvenient',\n",
              " 'working so changing review only thing works well if re honest bit could updated meals is painful',\n",
              " 'able benefit data points of data points useless wear watch not intend do workout heart monitor arm get data',\n",
              " 'update removed identifier watch syncing of with than watch is downgrade hope next update returns handy feature blood glucose recording needs place notes measurement ability edit day time going be truly useful be able note feeling medications etc',\n",
              " 'weeks app phone watches don match phone more should calories burned activity minutes other hand synchronization watches only correct of steps only few calories minutes though finished fitness exercises',\n",
              " 'helpful app major problem transitions recently watch caught sleep correctly app interpreted data incorrectly describe messed sleep record suffice chart bars 42 night recorded sleep shows app isn counted app app total says manually adding actual hours calculates to almost',\n",
              " 'use want use smart watch night have manually tracking sleep app yrs pretty straightforward enter time slept rate great senses using phone estimate sleep great when works reason rarely and understand hard figure out could fix give stars way step tracker works great',\n",
              " 'app good logging exercises absolutely useless food logging ve tracked food multiple different apps know well makes tacking easy useful intuitive health non food tracking barcode scanner horrible dial takes forever change quantity information products micros macros way split food different times day',\n",
              " 'turned notifications getting annoying notifications decent app to delete keeps up ve app years recent forced upgrade implemented few weeks ago HATE new interface changes nearly',\n",
              " 'horribly arranged exorcism setting long term tends not manually edit types workouts fact exercises far limited ability scan barcodes calorie logging go submenu add meal scroll tap entries places coarse exercise mispronounces words like',\n",
              " 'now watch hate app more m going to fitbit']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "united_string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-oeT3ILTreN"
      },
      "source": [
        "Apply SAFE POS pattern"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2lciKH5WTy0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf33f15e-9437-4a2c-b4a0-d6626a12e4f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good selection workout options\n",
            "wish option add note\n",
            "\n",
            "good job tracking sleep\n",
            "\n",
            "more breathing cycles section\n",
            "accurate jnside apartment building\n",
            "building\n",
            "walking halls near parameter\n",
            "little ways instance scheduling\n",
            "\n",
            "good meassure steps distance\n",
            "enter data back sleep\n",
            "best feature fact review\n",
            "route feature\n",
            "disappointing app return map\n",
            "longer workout notifications steps\n",
            "past week use galaxy\n",
            "won load calendar longer\n",
            "showing on main screen\n",
            "turn phone for minute\n",
            "let sign with account\n",
            "wish edit time sleep\n",
            "nutritional information item search\n",
            "total sleep time graph\n",
            "old phone monitor heart\n",
            "best app fitness hands\n",
            "main function app weight\n",
            "inconsistent records sleep exercise\n",
            "proper stretching practice idea\n",
            "decent job tracking activity\n",
            "reminder automatic sleep detection\n",
            "automatic activity tracker sleep\n",
            "able benefit data points\n",
            "handy feature blood glucose\n",
            "next update returns recording\n",
            "non food tracking barcode\n",
            "coarse exercise mispronounces words\n",
            "[good selection workout options, wish option add note, , good job tracking sleep, , more breathing cycles section, accurate jnside apartment building, building, walking halls near parameter, little ways instance scheduling, , good meassure steps distance, enter data back sleep, best feature fact review, route feature, disappointing app return map, longer workout notifications steps, past week use galaxy, won load calendar longer, showing on main screen, turn phone for minute, let sign with account, wish edit time sleep, nutritional information item search, total sleep time graph, old phone monitor heart, best app fitness hands, main function app weight, inconsistent records sleep exercise, proper stretching practice idea, decent job tracking activity, reminder automatic sleep detection, automatic activity tracker sleep, able benefit data points, handy feature blood glucose, next update returns recording, non food tracking barcode, coarse exercise mispronounces words]\n"
          ]
        }
      ],
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher1 = Matcher(nlp.vocab)\n",
        "matcher2 = Matcher(nlp.vocab)\n",
        "matcher3 = Matcher(nlp.vocab)\n",
        "\n",
        "\"\"\"def deleteTokens(matcher, doc, i, matches):\n",
        "    # Get the current match and create tuple of entity label, start and end.\n",
        "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
        "    match_id, start, end = matches[i]\n",
        "    string_id = nlp.vocab.strings[match_id]\n",
        "    text = \"\"\n",
        "\n",
        "    nlp_list = list(doc)\n",
        "    for i in range(end-start):\n",
        "        text += nlp_list[start].text + \" \"\n",
        "        del nlp_list[start]\n",
        "    print(\"\\n\")\n",
        "    print(text)\n",
        "    print(\"\\n\")\n",
        "    print(nlp_list)\n",
        "    doc = nlp(\" \".join([e.text for e in nlp_list]))\n",
        "    matched_list.append(text)\n",
        "    matched_id_list.append(string_id)\"\"\"\n",
        "\n",
        "\n",
        "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
        "pattern1 = [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern2 = [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern3 = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern4 = [{\"POS\": \"NOUN\"}, {\"POS\": \"CCONJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern5 = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern6 = [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern7 = [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern8 = [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern9 = [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern10 = [{\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern11 = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADP\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern12 = [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern13 = [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"ADP\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern14 = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern15 = [{\"POS\": \"ADJ\"}, {\"POS\": \"CONJ\"}, {\"POS\": \"ADJ\"}]\n",
        "pattern16 = [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern17 = [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern18 = [{\"POS\": \"NOUN\"}, {\"POS\": \"CCONJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "matcher1.add(\"18\", [pattern18])\n",
        "matcher1.add(\"17\", [pattern17])\n",
        "matcher1.add(\"16\", [pattern16])\n",
        "matcher2.add(\"15\", [pattern15])\n",
        "matcher1.add(\"14\", [pattern14])\n",
        "matcher1.add(\"13\", [pattern13])\n",
        "matcher2.add(\"12\", [pattern12])\n",
        "matcher2.add(\"11\", [pattern11])\n",
        "matcher2.add(\"10\", [pattern10])\n",
        "matcher2.add(\"9\", [pattern9])\n",
        "matcher2.add(\"8\", [pattern8])\n",
        "matcher2.add(\"7\", [pattern7])\n",
        "matcher2.add(\"6\", [pattern6])\n",
        "matcher2.add(\"5\", [pattern5])\n",
        "matcher2.add(\"4\", [pattern4])\n",
        "matcher3.add(\"3\", [pattern3])\n",
        "matcher3.add(\"2\", [pattern2])\n",
        "matcher3.add(\"1\", [pattern1])\n",
        "\n",
        "matched_id_list = []\n",
        "matched_list = []\n",
        "#doc = nlp(\"Song and artist album,  Create your greatest album, \")\n",
        "\n",
        "for row in range(len(united_string)):\n",
        "  doc = nlp(united_string[row])\n",
        "  while(True):\n",
        "    matches = matcher1(doc)\n",
        "    for match_id, start, end in matches:\n",
        "      string_id = nlp.vocab.strings[match_id]\n",
        "      nlp_list = list(doc)\n",
        "      print(doc[start:end])\n",
        "      matched_list.append(doc[start:end])\n",
        "      matched_id_list.append(string_id)\n",
        "\n",
        "      for i in range(end-start):\n",
        "        if(start<len(nlp_list)):\n",
        "          del nlp_list[start]\n",
        "\n",
        "      doc = nlp(\" \".join([e.text for e in nlp_list]))\n",
        "    if (not matches):\n",
        "      break\n",
        "  while(True):\n",
        "    matches = matcher1(doc)\n",
        "    for match_id, start, end in matches:\n",
        "      string_id = nlp.vocab.strings[match_id]\n",
        "      nlp_list = list(doc)\n",
        "      print(doc[start:end])\n",
        "      matched_list.append(doc[start:end])\n",
        "      matched_id_list.append(string_id)\n",
        "\n",
        "      for i in range(end-start):\n",
        "        if(start<len(nlp_list)):\n",
        "          del nlp_list[start]\n",
        "\n",
        "      doc = nlp(\" \".join([e.text for e in nlp_list]))\n",
        "    if (not matches):\n",
        "      break\n",
        "  while(True):\n",
        "    matches = matcher1(doc)\n",
        "    for match_id, start, end in matches:\n",
        "      string_id = nlp.vocab.strings[match_id]\n",
        "      nlp_list = list(doc)\n",
        "      print(doc[start:end])\n",
        "      matched_list.append(doc[start:end])\n",
        "      matched_id_list.append(string_id)\n",
        "\n",
        "      for i in range(end-start):\n",
        "        if(start<len(nlp_list)):\n",
        "          del nlp_list[start]\n",
        "\n",
        "      doc = nlp(\" \".join([e.text for e in nlp_list]))\n",
        "    if (not matches):\n",
        "      break\n",
        "\n",
        "\n",
        "#Remove duplicate and synonyms, noise\n",
        "print(matched_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "RHiRNh4gr2pK"
      },
      "outputs": [],
      "source": [
        "for el in matched_list:\n",
        "  if len(el.text.split(\" \"))<=1:\n",
        "    matched_list.remove(el)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "o4jbgN19tDpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f53dfd84-a6de-4ea1-9a89-6875cb790b9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[good selection workout options,\n",
              " wish option add note,\n",
              " good job tracking sleep,\n",
              " more breathing cycles section,\n",
              " accurate jnside apartment building,\n",
              " walking halls near parameter,\n",
              " little ways instance scheduling,\n",
              " good meassure steps distance,\n",
              " enter data back sleep,\n",
              " best feature fact review,\n",
              " route feature,\n",
              " disappointing app return map,\n",
              " longer workout notifications steps,\n",
              " past week use galaxy,\n",
              " won load calendar longer,\n",
              " showing on main screen,\n",
              " turn phone for minute,\n",
              " let sign with account,\n",
              " wish edit time sleep,\n",
              " nutritional information item search,\n",
              " total sleep time graph,\n",
              " old phone monitor heart,\n",
              " best app fitness hands,\n",
              " main function app weight,\n",
              " inconsistent records sleep exercise,\n",
              " proper stretching practice idea,\n",
              " decent job tracking activity,\n",
              " reminder automatic sleep detection,\n",
              " automatic activity tracker sleep,\n",
              " able benefit data points,\n",
              " handy feature blood glucose,\n",
              " next update returns recording,\n",
              " non food tracking barcode,\n",
              " coarse exercise mispronounces words]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "matched_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "pY456lm7Maow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "# Define the list of sentences\n",
        "sentences = matched_list\n",
        "\n",
        "# Load a pre-trained sentence-transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Compute the sentence embeddings\n",
        "sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "nbrs = NearestNeighbors(n_neighbors=2).fit(sentence_embeddings)\n",
        "distances, _ = nbrs.kneighbors(sentence_embeddings)\n",
        "eps = max(distances[:, 1])\n",
        "\n",
        "# Cluster the sentence embeddings using DBSCAN\n",
        "dbscan = DBSCAN(eps=eps, min_samples=2)\n",
        "clusters = dbscan.fit_predict(sentence_embeddings)\n",
        "\n",
        "print(max(clusters))\n",
        "# Print the clusters and their corresponding sentences\n",
        "for i in range(max(clusters)+1):\n",
        "  print(f\"i: {i}\")\n",
        "  print(f\"Cluster {i+1}:\")\n",
        "  for sentence in sentences[clusters==i]:\n",
        "      print(f\"- {sentence}\")\n",
        "      print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "PYDOSkkYHM7F",
        "outputId": "553d4da8-84f2-4a45-9c28-b171342ec20e"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "i: 0\n",
            "Cluster 1:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-3178209b85ac>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"i: {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Cluster {i+1}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"- {sentence}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load pre-trained language model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Define number of clusters\n",
        "num_clusters = 7\n",
        "\n",
        "# Define the list of phrases to cluster\n",
        "phrases = matched_list\n",
        "\n",
        "# Encode the phrases using the pre-trained model\n",
        "phrase_embeddings = model.encode(phrases)\n",
        "\n",
        "# Cluster the embeddings using K-means\n",
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(phrase_embeddings)\n",
        "\n",
        "# Assign each phrase to a cluster\n",
        "clusters = [[] for i in range(num_clusters)]\n",
        "for i, label in enumerate(kmeans.labels_):\n",
        "    clusters[label].append(phrases[i])\n",
        "\n",
        "# Print the clusters\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster {}:\".format(i+1))\n",
        "    print(clusters[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouXR8d26aMi6",
        "outputId": "5451a1c8-abb0-45a4-da76-cc44a74c127d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 1:\n",
            "[good selection workout options, good job tracking sleep, accurate jnside apartment building, good meassure steps distance, decent job tracking activity]\n",
            "Cluster 2:\n",
            "[more breathing cycles section, longer workout notifications steps, total sleep time graph, proper stretching practice idea, coarse exercise mispronounces words]\n",
            "Cluster 3:\n",
            "[wish option add note, won load calendar longer, showing on main screen, let sign with account, wish edit time sleep, next update returns recording]\n",
            "Cluster 4:\n",
            "[disappointing app return map, past week use galaxy, turn phone for minute, old phone monitor heart, best app fitness hands, inconsistent records sleep exercise]\n",
            "Cluster 5:\n",
            "[best feature fact review, route feature, main function app weight, reminder automatic sleep detection, automatic activity tracker sleep, able benefit data points, handy feature blood glucose]\n",
            "Cluster 6:\n",
            "[enter data back sleep, nutritional information item search, non food tracking barcode]\n",
            "Cluster 7:\n",
            "[walking halls near parameter, little ways instance scheduling]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = matched_list\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Normalize the embeddings to unit length\n",
        "corpus_embeddings = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Set up Faiss for nearest neighbor search\n",
        "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])\n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "# Perform nearest neighbor search to get indices of similar embeddings\n",
        "_, indices = index.search(corpus_embeddings, k=5)  # Get the 5 most similar embeddings for each sentence\n",
        "\n",
        "# Perform DBSCAN clustering on the similar embeddings\n",
        "clustering_model = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
        "cluster_assignment = clustering_model.fit_predict(corpus_embeddings[indices[:, 1]])\n",
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in clustered_sentences.items():\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMmbWbtxa9fJ",
        "outputId": "56dbcd6c-8080-429d-f403-2379ddf9f238"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster  1\n",
            "[good selection workout options, good job tracking sleep, good meassure steps distance, decent job tracking activity]\n",
            "\n",
            "Cluster  2\n",
            "[wish option add note, wish edit time sleep, next update returns recording]\n",
            "\n",
            "Cluster  3\n",
            "[more breathing cycles section, proper stretching practice idea, coarse exercise mispronounces words]\n",
            "\n",
            "Cluster  4\n",
            "[accurate jnside apartment building, best feature fact review, route feature, disappointing app return map, best app fitness hands, able benefit data points, handy feature blood glucose]\n",
            "\n",
            "Cluster  0\n",
            "[walking halls near parameter, enter data back sleep, nutritional information item search, non food tracking barcode]\n",
            "\n",
            "Cluster  5\n",
            "[little ways instance scheduling, longer workout notifications steps, total sleep time graph]\n",
            "\n",
            "Cluster  6\n",
            "[past week use galaxy, turn phone for minute, old phone monitor heart]\n",
            "\n",
            "Cluster  7\n",
            "[won load calendar longer, showing on main screen, let sign with account]\n",
            "\n",
            "Cluster  8\n",
            "[main function app weight, inconsistent records sleep exercise]\n",
            "\n",
            "Cluster  9\n",
            "[reminder automatic sleep detection, automatic activity tracker sleep]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is a simple application for sentence embeddings: clustering\n",
        "Sentences are mapped to sentence embeddings and then k-mean clustering is applied.\n",
        "\"\"\"\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = matched_list\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Perform kmean clustering\n",
        "num_clusters = 6\n",
        "clustering_model = KMeans(n_clusters=num_clusters)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = [[] for i in range(num_clusters)]\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in enumerate(clustered_sentences):\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCm_99BgkdDa",
        "outputId": "7427dacd-13dc-4025-d0ae-e6d921eed9af"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster  1\n",
            "[good selection workout options, good job tracking sleep, accurate jnside apartment building, good meassure steps distance, decent job tracking activity, next update returns recording]\n",
            "\n",
            "Cluster  2\n",
            "[wish option add note, enter data back sleep, won load calendar longer, showing on main screen, let sign with account, wish edit time sleep, main function app weight, inconsistent records sleep exercise]\n",
            "\n",
            "Cluster  3\n",
            "[best feature fact review, route feature, reminder automatic sleep detection, automatic activity tracker sleep, handy feature blood glucose]\n",
            "\n",
            "Cluster  4\n",
            "[more breathing cycles section, walking halls near parameter, little ways instance scheduling, longer workout notifications steps, total sleep time graph, proper stretching practice idea, able benefit data points, coarse exercise mispronounces words]\n",
            "\n",
            "Cluster  5\n",
            "[disappointing app return map, past week use galaxy, turn phone for minute, old phone monitor heart, best app fitness hands]\n",
            "\n",
            "Cluster  6\n",
            "[nutritional information item search, non food tracking barcode]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This is a simple application for sentence embeddings: clustering\n",
        "\n",
        "Sentences are mapped to sentence embeddings and then agglomerative clustering with a threshold is applied.\n",
        "\"\"\"\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = matched_list\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Normalize the embeddings to unit length\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Perform kmean clustering\n",
        "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) #, affinity='cosine', linkage='average', distance_threshold=0.4)\n",
        "clustering_model.fit(corpus_embeddings)\n",
        "cluster_assignment = clustering_model.labels_\n",
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in clustered_sentences.items():\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_O88qI8lBuz",
        "outputId": "00564f8d-c2e3-4077-f14b-b5954d10db90"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster  5\n",
            "[good selection workout options, good job tracking sleep, good meassure steps distance, decent job tracking activity]\n",
            "\n",
            "Cluster  1\n",
            "[wish option add note, past week use galaxy, won load calendar longer, showing on main screen, turn phone for minute, let sign with account, wish edit time sleep, old phone monitor heart, next update returns recording]\n",
            "\n",
            "Cluster  4\n",
            "[more breathing cycles section, walking halls near parameter, little ways instance scheduling, longer workout notifications steps, total sleep time graph, proper stretching practice idea, coarse exercise mispronounces words]\n",
            "\n",
            "Cluster  3\n",
            "[accurate jnside apartment building, best feature fact review, route feature, disappointing app return map, best app fitness hands, able benefit data points, handy feature blood glucose]\n",
            "\n",
            "Cluster  2\n",
            "[enter data back sleep, nutritional information item search, main function app weight, inconsistent records sleep exercise, non food tracking barcode]\n",
            "\n",
            "Cluster  6\n",
            "[reminder automatic sleep detection, automatic activity tracker sleep]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = matched_list\n",
        "corpus_embeddings = embedder.encode(corpus)\n",
        "\n",
        "# Normalize the embeddings to unit length\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Perform DBSCAN clustering\n",
        "clustering_model = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
        "cluster_assignment = clustering_model.fit_predict(corpus_embeddings)\n",
        "\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences:\n",
        "        clustered_sentences[cluster_id] = []\n",
        "\n",
        "    clustered_sentences[cluster_id].append(corpus[sentence_id])\n",
        "\n",
        "for i, cluster in clustered_sentences.items():\n",
        "    print(\"Cluster \", i+1)\n",
        "    print(cluster)\n",
        "    print(\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tx7Ad_ESoj10",
        "outputId": "488dcc95-448f-4099-a86b-d5c44667d09f"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster  0\n",
            "[good selection workout options, more breathing cycles section, accurate jnside apartment building, walking halls near parameter, little ways instance scheduling, good meassure steps distance, enter data back sleep, longer workout notifications steps, past week use galaxy, won load calendar longer, showing on main screen, let sign with account, total sleep time graph, main function app weight, inconsistent records sleep exercise, able benefit data points, next update returns recording]\n",
            "\n",
            "Cluster  1\n",
            "[wish option add note, wish edit time sleep]\n",
            "\n",
            "Cluster  2\n",
            "[good job tracking sleep, decent job tracking activity]\n",
            "\n",
            "Cluster  3\n",
            "[best feature fact review, route feature, disappointing app return map, best app fitness hands, handy feature blood glucose]\n",
            "\n",
            "Cluster  4\n",
            "[turn phone for minute, old phone monitor heart]\n",
            "\n",
            "Cluster  5\n",
            "[nutritional information item search, non food tracking barcode]\n",
            "\n",
            "Cluster  6\n",
            "[proper stretching practice idea, coarse exercise mispronounces words]\n",
            "\n",
            "Cluster  7\n",
            "[reminder automatic sleep detection, automatic activity tracker sleep]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "MidEnzxW81Zi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "481e6a8b-ca9c-4613-874b-234325696259"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-59cb780bbba1>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# We use cosine-similarity and torch.topk to find the highest 5 scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mlength_sorted_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0msentences_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlength_sorted_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mlength_sorted_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0msentences_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlength_sorted_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36m_text_length\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0;31m#Object has no len() method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m#Empty string or list of ints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.token.Token' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This is a simple application for sentence embeddings: semantic search\n",
        "\n",
        "We have a corpus with various sentences. Then, for a given query sentence,\n",
        "we want to find the most similar sentence in this corpus.\n",
        "\n",
        "This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
        "\"\"\"\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.cluster import dbscan\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = matched_list[1:]\n",
        "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "# Query sentences:\n",
        "queries = [matched_list[0]]\n",
        "\n",
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "top_k = min(5, len(corpus))\n",
        "for query in queries:\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\n",
        "\n",
        "    \"\"\"\n",
        "    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
        "    hits = hits[0]      #Get the hits for the first query\n",
        "    for hit in hits:\n",
        "        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "B9evoOemLW6u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "3f279f60-bd44-4fe1-ba9a-7db35d33ee00"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-1a11a9ec5789>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# We use cosine-similarity and torch.topk to find the highest 5 scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mall_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mlength_sorted_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_text_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msen\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0msentences_sorted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlength_sorted_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'spacy.tokens.token.Token' object is not iterable"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Corpus with example sentences\n",
        "corpus = matched_list[1:]\n",
        "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "\n",
        "# Query sentences:\n",
        "queries = [matched_list[0]]\n",
        "\n",
        "for queries in matched_list:\n",
        "  # Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "  top_k = min(100, len(corpus))\n",
        "  for query in queries:\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "        if(score > 0,6):\n",
        "          matched_list.pop(idx)\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "3z2YiPbAog0B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b49bc8ab-4eeb-48aa-a28a-b9040c0a5557"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "#On charge le modèle de transformation de phrases \"all-MiniLM-L6-v2\" à partir de la bibliothèque SentenceTransformer.\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#le nombre de voisins les plus proches à récupérer.\n",
        "# Define the number of nearest neighbors to retrieve\n",
        "k = 5\n",
        "\n",
        "# Define the corpus of sentences\n",
        "corpus = matched_list[1:]\n",
        "\n",
        "# Encoder le corpus de phrases en utilisant le modèle de transformation de phrases.\n",
        "corpus_embeddings = model.encode(corpus)\n",
        "\n",
        "# Embed the query sentence\n",
        "query = matched_list[0]\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Create an index for the corpus embeddings\n",
        "#On crée un index FAISS pour les embeddings des phrases dans le corpus,\n",
        "#en utilisant l'algorithme \"IndexFlatIP\" pour calculer le produit scalaire entre les embeddings.\n",
        "d = len(corpus_embeddings[0])\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(np.array(corpus_embeddings))\n",
        "\n",
        "# Search for the k nearest neighbors\n",
        "D, I = index.search(np.array(query_embedding), k)\n",
        "\n",
        "# Print the results\n",
        "print(\"\\n\\n======================\\n\\n\")\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "for i, idx in enumerate(I[0]):\n",
        "    print(corpus[idx], \"(Score: {:.4f})\".format(D[0][i]))"
      ],
      "metadata": {
        "id": "0hE0U9VkoqP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d036b4a-ad40-4051-9705-2793db7ece7a"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: good selection workout options\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "good meassure steps distance (Score: 0.4357)\n",
            "good job tracking sleep (Score: 0.4207)\n",
            "best feature fact review (Score: 0.4139)\n",
            "decent job tracking activity (Score: 0.4117)\n",
            "wish option add note (Score: 0.4072)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "#On charge le modèle de transformation de phrases \"all-MiniLM-L6-v2\" à partir de la bibliothèque SentenceTransformer.\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#le nombre de voisins les plus proches à récupérer.\n",
        "# Define the number of nearest neighbors to retrieve\n",
        "k = 5\n",
        "\n",
        "# Define the corpus of sentences\n",
        "corpus = matched_list[1:]\n",
        "\n",
        "# Encoder le corpus de phrases en utilisant le modèle de transformation de phrases.\n",
        "corpus_embeddings = model.encode(corpus)\n",
        "\n",
        "# Embed the query sentence\n",
        "query = matched_list[0]\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Create an index for the corpus embeddings\n",
        "#On crée un index FAISS pour les embeddings des phrases dans le corpus,\n",
        "#en utilisant l'algorithme \"IndexFlatIP\" pour calculer le produit scalaire entre les embeddings.\n",
        "d = len(corpus_embeddings[0])\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(np.array(corpus_embeddings))\n",
        "\n",
        "# Search for the k nearest neighbors\n",
        "D, I = index.search(np.array(query_embedding), k)\n",
        "\n",
        "# Get the features to be clustered\n",
        "features = np.array(matched_list)\n",
        "\n",
        "# Define the parameters for the DBSCAN clustering algorithm\n",
        "eps = 0.5\n",
        "min_samples = 5\n",
        "\n",
        "# Instantiate a DBSCAN object with the defined parameters\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "\n",
        "# Fit the DBSCAN object to the features\n",
        "dbscan.fit(features)\n",
        "\n",
        "# Get the labels for each point in the features\n",
        "labels = dbscan.labels_\n",
        "\n",
        "# Get the unique labels (i.e., cluster IDs) in the labels array\n",
        "unique_labels = np.unique(labels)\n",
        "\n",
        "# Loop through each label to print the corresponding features\n",
        "for label in unique_labels:\n",
        "    if label == -1:\n",
        "        # This is noise, i.e., a feature that doesn't belong to any cluster\n",
        "        print(\"Noise:\")\n",
        "    else:\n",
        "        # This is a cluster\n",
        "        print(\"Cluster\", label, \":\")\n",
        "    # Print the features in the current cluster\n",
        "    for idx, feature in enumerate(features[labels == label]):\n",
        "        print(\"\\t\", idx+1, \":\", feature)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "MMR0KtF7XjQG",
        "outputId": "62c69f38-4fed-4fc1-f192-9d55feac1291"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-f7bdf9137db1>:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  features = np.array(matched_list)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'spacy.tokens.span.Span'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-f7bdf9137db1>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Fit the DBSCAN object to the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# Get the labels for each point in the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/cluster/_dbscan.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wKMu4ESEYWlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "#On charge le modèle de transformation de phrases \"all-MiniLM-L6-v2\" à partir de la bibliothèque SentenceTransformer.\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "#le nombre de voisins les plus proches à récupérer.\n",
        "# Define the number of nearest neighbors to retrieve\n",
        "k = 5\n",
        "\n",
        "# Define the corpus of sentences\n",
        "corpus = matched_list[1:]\n",
        "\n",
        "# Encoder le corpus de phrases en utilisant le modèle de transformation de phrases.\n",
        "corpus_embeddings = model.encode(corpus)\n",
        "\n",
        "# Embed the query sentence\n",
        "query = matched_list[0]\n",
        "query_embedding = model.encode([query])\n",
        "\n",
        "# Create an index for the corpus embeddings\n",
        "#On crée un index FAISS pour les embeddings des phrases dans le corpus,\n",
        "#en utilisant l'algorithme \"IndexFlatIP\" pour calculer le produit scalaire entre les embeddings.\n",
        "d = len(corpus_embeddings[0])\n",
        "index = faiss.IndexFlatIP(d)\n",
        "index.add(np.array(corpus_embeddings))\n",
        "\n",
        "print(index)\n",
        "\n",
        "# Search for the k nearest neighbors\n",
        "D, I = index.search(np.array(query_embedding), k)\n",
        "\n",
        "# Print the results\n",
        "print(\"\\n\\n======================\\n\\n\")\n",
        "print(\"Query:\", query)\n",
        "print(\"\\nTop 5 most similar sentences in corpus:\")\n",
        "for i, idx in enumerate(I[0]):\n",
        "    print(corpus[idx], \"(Score: {:.4f})\".format(D[0][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqgV6nhLh2Lh",
        "outputId": "69f3151f-75d4-4501-d37b-aa22d02e2db7"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<faiss.swigfaiss_avx2.IndexFlatIP; proxy of <Swig Object of type 'faiss::IndexFlatIP *' at 0x7f02aac98c30> >\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: good selection workout options\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "good meassure steps distance (Score: 0.4357)\n",
            "good job tracking sleep (Score: 0.4207)\n",
            "best feature fact review (Score: 0.4139)\n",
            "decent job tracking activity (Score: 0.4117)\n",
            "wish option add note (Score: 0.4072)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}