{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Chargement des données csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRP7dufeAThK",
        "outputId": "4bdb0381-a191-44ea-8a10-0366d62dddbf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #import de la bibliothèque pandas\n",
        "\n",
        "path = \"GooglePlayReviewsEN.csv\" #import du fichier csv \n",
        "df_content = pd.read_csv(path) #lecture du fichier csv"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QIoLc4EtUxYi"
      },
      "source": [
        "Lecture du fichier csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IsOHf05WUw0U"
      },
      "outputs": [],
      "source": [
        "import csv #import de la bibliothèque csv\n",
        "\n",
        "# ouverture du fichier csv en mode lecture \n",
        "with open(path, newline='',  encoding='utf-8') as csvfile:\n",
        "    # création d'un objet reader qui va lire le fichier csv\n",
        "    reader = csv.reader(csvfile)\n",
        "    # création d'une liste qui va contenir les lignes du fichier csv\n",
        "    string_list = []\n",
        "    # boucle sur les lignes du fichier csv\n",
        "    for row in reader:\n",
        "        # ajout de la ligne dans la liste\n",
        "        string_list.append(row[0])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Segmentation des données de la liste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "string_list = string_list[:1000] #on ne prend que les 1000 premiers commentaires"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Traitement des données sur le fichier jsonlines"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install jsonlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDyCue2PrD3Z",
        "outputId": "78522c07-bb8a-40ad-bb46-f3cfc8dcec65"
      },
      "outputs": [],
      "source": [
        "#import jsonl & fichier annotations\n",
        "import jsonlines\n",
        "\n",
        "path = \"AnnotedComments.jsonl\"\n",
        "annotations =[]\n",
        "\n",
        "\n",
        "with jsonlines.open(path) as reader:\n",
        "  i = 0\n",
        "  for obj in reader:\n",
        "    annotation = [i]\n",
        "    for note in obj['entities']:\n",
        "      annotation.append([note['start_offset'], note['end_offset']])\n",
        "      print(annotation)\n",
        "    if obj['entities']:\n",
        "      annotations.append(annotation)\n",
        "    i += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3GNJuYWTitA"
      },
      "source": [
        "Pre-traitement du texte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AucR-PxMW9Mf",
        "outputId": "68c1c491-35c2-4c03-a0f2-e92ec14102af"
      },
      "outputs": [],
      "source": [
        "# import de la bibliothèque spacy \n",
        "import spacy\n",
        "# import de la bibliothèque re \n",
        "import re\n",
        "\n",
        "# chargement du modèle en_core_web_sm \n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# chargement du modèle anglais de spacy \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# liste des mots d'arrêt les plus courants en anglais\n",
        "listStopWords = [str(x) for x in nlp.Defaults.stop_words]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Création de la fonction de pré-traitement des textes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "K0s-fFW2TgdN"
      },
      "outputs": [],
      "source": [
        "def united(text):\n",
        "\n",
        "    \"\"\"Suppression des explications\"\"\"\n",
        "    doc =  text.split(\" \");\n",
        "    # Itérer à travers chaque token dans le document\n",
        "    for token in doc:\n",
        "        # Si le token est \"parce que\", \"car\" ou \"puisque\", supprimer tous les tokens suivants dans la phrase\n",
        "        if token in [\"because\", \"since\", \"as\", ]:\n",
        "            # Supprimer tous les tokens après le token \"parce que\", \"car\" ou \"puisque\"\n",
        "            doc = doc[:doc.index(token)]\n",
        "            # Sortir de la boucle\n",
        "            break\n",
        "    \n",
        "    # Rejoindre les tokens dans la phrase\n",
        "    sentence = ' '.join(doc)\n",
        "    \"\"\"Fin de la suppression des explications\"\"\"\n",
        "\n",
        "    \"\"\"Suppression des caractères spéciaux\"\"\"\n",
        "    sentence = re.sub(r'\\([^()]*\\)', '', sentence) # Suppression des parenthèses\n",
        "\n",
        "    \"\"\"Suppression des liens web dans la phrase\"\"\"\n",
        "    sentence = re.sub('http[s]?://\\S+', '', sentence) # Suppression des liens web\n",
        "\n",
        "\n",
        "    \"\"\"Suppression des numéros de téléphone et des adresses e-mail\"\"\"\n",
        "    quotes = r'\\\"[^\\\"]+\\\"' # expression régulière pour les citations\n",
        "    phone_regex = r\"(?<!\\d)(?:\\d[ -/\\\\_\\d]*){10}(?!\\d)\" # Expression régulière pour les numéros de téléphone\n",
        "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"  # Expression régulière pour les adresses e-mail\n",
        "\n",
        "    sentence = re.sub(quotes,\"\", sentence) # Suppression des citations\n",
        "    sentence = re.sub(phone_regex,\"\", sentence) # Suppression des numéros de téléphone\n",
        "    sentence = re.sub(email_regex,\"\", sentence) # Suppression des adresses e-mail\n",
        "    sentence = re.sub(r'[^\\w]', ' ', sentence) # Suppression des caractères spéciaux\n",
        "\n",
        "    \"\"\"Suppression des mots d'arrêt\"\"\"\n",
        "    doc = sentence.split(\" \") # Séparer la phrase en tokens\n",
        "    for word in doc: # Itérer à travers chaque token dans la phrase\n",
        "        if word in listStopWords: # Si le token est un mot d'arrêt, supprimer le token\n",
        "            doc.remove(word) # Supprimer le token\n",
        "    \n",
        "    sentence = ' '.join(doc) # Rejoindre les tokens dans la phrase\n",
        "\n",
        "\n",
        "    \"\"\"Suppression des caractéres spéciaux \"'\" \"\"\"\n",
        "    sentence = sentence.replace(\"'\", \" \") # Remplacer les caractéres spéciaux \"'\" par un espace\n",
        "\n",
        "    \"\"\"Suppression des mots de moins de 2 caractères\"\"\"\n",
        "    words = sentence.split(\" \") # Séparer la phrase en tokens\n",
        "    newSentence = \"\" # Créer une nouvelle phrase vide\t\n",
        "    for word in words: # Itérer à travers chaque token dans la phrase\n",
        "        if len(word) > 1: # Si le token est plus grand que 1 caractère, ajouter le token à la nouvelle phrase\n",
        "            newSentence += word + \" \" # Ajouter le token à la nouvelle phrase\n",
        "    sentence = newSentence # Mettre la nouvelle phrase dans la variable sentence\n",
        "    \n",
        "    \"\"\"Suppression des pronoms et déterminants\"\"\"\n",
        "    doc = nlp(sentence) # Charger la phrase dans le modèle spacy\n",
        "    text = \"\" # Créer une nouvelle phrase vide\n",
        "    for token in doc: # Itérer à travers chaque token dans le document\n",
        "        if (token.pos_ == \"PRON\" or token.pos_ == \"DET\"): # Si le token est un pronom ou un déterminant, ignorer le token\n",
        "            continue # Ignorer le token\n",
        "        else: # Sinon, ajouter le token à la nouvelle phrase\n",
        "            if not token.text.__contains__(\"'\"): # Si le token ne contient pas le caractére spécial \"'\", ajouter le token à la nouvelle phrase\n",
        "                text += token.text + \" \" # Ajouter le token à la nouvelle phrase\n",
        "                text += \" \" # Ajouter un espace à la nouvelle phrase\n",
        "    sentence = text # Mettre la nouvelle phrase dans la variable sentence\n",
        "\n",
        "    \"\"\"Suppression des noms propres et des noms d'organisations\"\"\"\n",
        "    doc = nlp(sentence) # Charger la phrase dans le modèle spacy\n",
        "    result = [] # Créer une nouvelle phrase vide \n",
        "    for token in doc: # Itérer à travers chaque token dans le document\n",
        "        if not (token.ent_type_ == \"ORG\" or token.ent_type_ == \"PERSON\" or token.text.istitle()): # Si le token est un nom propre ou un nom d'organisation, ignorer le token\n",
        "            result.append(token.text) # Ajouter le token à la nouvelle phrase\n",
        "    sentence =  \" \".join(result) # Rejoindre les tokens dans la phrase\n",
        "\n",
        "    return sentence # Retourner la phrase nettoyée"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Application de la fonction de pré-traitement sur les données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "united_string = [] # Créer une liste vide\n",
        "for i in range(len(string_list)): # Itérer à travers chaque phrase dans la liste\n",
        "  united_string += [united(string_list[i])] # Ajouter la phrase nettoyée à la liste"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i-oeT3ILTreN"
      },
      "source": [
        "Application des patterns de la méthode SAFE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cDd82HGy1478"
      },
      "outputs": [],
      "source": [
        "def superpose(da, fa, db, fb):\n",
        "  return (da <= db <= fa | da <= fb <= fa | (db < da & fa < fb))\n",
        "\n",
        "\n",
        "def matchTokens(id, start, end):\n",
        "  both = 0\n",
        "  for labeled in annotations:\n",
        "    if id == labeled[0]:\n",
        "      for i in range(1, len(labeled)):\n",
        "        print(labeled[i][0], labeled[i][1], start, end)\n",
        "        if superpose(labeled[i][0], labeled[i][1], start, end):\n",
        "          both +=1\n",
        "          break\n",
        "  return both\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Création des matchers de spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lciKH5WTy0D",
        "outputId": "8fc73992-b3eb-43cf-82f1-0313d27a72f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n"
          ]
        }
      ],
      "source": [
        "from spacy.matcher import Matcher # import matcher \n",
        "\n",
        "matcher1 = Matcher(nlp.vocab) # création d'un matcher \n",
        "matcher2 = Matcher(nlp.vocab) # création d'un matcher\n",
        "matcher3 = Matcher(nlp.vocab) # création d'un matcher\n",
        "\n",
        "\n",
        "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
        "pattern1 = [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern2 = [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern3 = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern4 = [{\"POS\": \"NOUN\"}, {\"POS\": \"CCONJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern5 = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern6 = [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern7 = [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern8 = [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern9 = [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern10 = [{\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern11 = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADP\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern12 = [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern13 = [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"ADP\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern14 = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern15 = [{\"POS\": \"ADJ\"}, {\"POS\": \"CONJ\"}, {\"POS\": \"ADJ\"}]\n",
        "pattern16 = [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern17 = [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
        "pattern18 = [{\"POS\": \"NOUN\"}, {\"POS\": \"CCONJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
        "matcher1.add(\"18\", [pattern18])\n",
        "matcher1.add(\"17\", [pattern17])\n",
        "matcher1.add(\"16\", [pattern16])\n",
        "matcher2.add(\"15\", [pattern15])\n",
        "matcher1.add(\"14\", [pattern14])\n",
        "matcher1.add(\"13\", [pattern13])\n",
        "matcher2.add(\"12\", [pattern12])\n",
        "matcher2.add(\"11\", [pattern11])\n",
        "matcher2.add(\"10\", [pattern10])\n",
        "matcher2.add(\"9\", [pattern9])\n",
        "matcher2.add(\"8\", [pattern8])\n",
        "matcher2.add(\"7\", [pattern7])\n",
        "matcher2.add(\"6\", [pattern6])\n",
        "matcher2.add(\"5\", [pattern5])\n",
        "matcher2.add(\"4\", [pattern4])\n",
        "matcher3.add(\"3\", [pattern3])\n",
        "matcher3.add(\"2\", [pattern2])\n",
        "matcher3.add(\"1\", [pattern1])\n",
        "\n",
        "matched_id_list = []\n",
        "matched_list = []\n",
        "#doc = nlp(\"Song and artist album,  Create your greatest album, \")\n",
        "\n",
        "for row in range(len(united_string)):\n",
        "  doc = nlp(united_string[row])\n",
        "  while(True):\n",
        "    matches = matcher1(doc)\n",
        "    for match_id, start, end in matches:\n",
        "      trupos = matchTokens(row, start, end)\n",
        "      string_id = nlp.vocab.strings[match_id]\n",
        "      nlp_list = list(doc)\n",
        "      #print(doc[start:end])\n",
        "      matched_list.append(doc[start:end])\n",
        "      matched_id_list.append(string_id)\n",
        "\n",
        "      for i in range(end-start):\n",
        "        if(start<len(nlp_list)):\n",
        "          del nlp_list[start]\n",
        "\n",
        "      doc = nlp(\" \".join([e.text for e in nlp_list]))\n",
        "    if (not matches):\n",
        "      break\n",
        "  while(True):\n",
        "    matches = matcher1(doc)\n",
        "    #trupos += matchTokens(row, matches)\n",
        "    for match_id, start, end in matches:\n",
        "      string_id = nlp.vocab.strings[match_id]\n",
        "      nlp_list = list(doc)\n",
        "      print(doc[start:end])\n",
        "      matched_list.append(doc[start:end])\n",
        "      matched_id_list.append(string_id)\n",
        "\n",
        "      for i in range(end-start):\n",
        "        if(start<len(nlp_list)):\n",
        "          del nlp_list[start]\n",
        "\n",
        "      doc = nlp(\" \".join([e.text for e in nlp_list]))\n",
        "    if (not matches):\n",
        "      break\n",
        "  while(True):\n",
        "    matches = matcher1(doc)\n",
        "    #trupos += matchTokens(row, matches)\n",
        "    for match_id, start, end in matches:\n",
        "      string_id = nlp.vocab.strings[match_id]\n",
        "      nlp_list = list(doc)\n",
        "      print(doc[start:end])\n",
        "      matched_list.append(doc[start:end])\n",
        "      matched_id_list.append(string_id)\n",
        "\n",
        "      for i in range(end-start):\n",
        "        if(start<len(nlp_list)):\n",
        "          del nlp_list[start]\n",
        "\n",
        "      doc = nlp(\" \".join([e.text for e in nlp_list]))\n",
        "    if (not matches):\n",
        "      break\n",
        "\n",
        "#print(trupos)\n",
        "#Remove duplicate and synonyms, noise\n",
        "print(matched_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RHiRNh4gr2pK"
      },
      "outputs": [],
      "source": [
        "for el in matched_list:\n",
        "  if len(el.text.split(\" \"))<=1:\n",
        "    matched_list.remove(el)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Affichage graphique de thème des caractéristiques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matched_list_str = [str(k) for k in matched_list] # Conversion des phrases en string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer # import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation # import LatentDirichletAllocation\n",
        "import matplotlib.pyplot as plt # import matplotlib\n",
        "import networkx as nx # import networkx\n",
        "\n",
        "# Liste des caractéristiques à traiter\n",
        "features = matched_list_str[:100]  # 100 caractéristiques à traiter\n",
        "\n",
        "# Preprocessing des données\n",
        "preprocessed_features = [feat.lower().split() for feat in features]\n",
        "\n",
        "# Création de la matrice de termes\n",
        "vectorizer = CountVectorizer()\n",
        "dtm = vectorizer.fit_transform([' '.join(feat) for feat in preprocessed_features]) # Création de la matrice de termes\n",
        "\n",
        "# Identification des thèmes de la matrice de termes\n",
        "n_topics = 10 # Nombre de thèmes\n",
        "lda = LatentDirichletAllocation(n_components=n_topics, random_state=0) # Création du modèle LDA\n",
        "doc_topic_dist = lda.fit_transform(dtm) # Identification des thèmes de la matrice de termes\n",
        "\n",
        "# Création du graphe des thèmes\n",
        "G = nx.Graph() # Création du graphe\n",
        "for i, feat in enumerate(features): # Pour chaque caractéristique\n",
        "    topic = doc_topic_dist[i].argmax() # Récupération du thème de la caractéristique\n",
        "    G.add_node(feat, topic=topic, prob=doc_topic_dist[i][topic]) # Ajout du noeud correspondant à la caractéristique\n",
        "\n",
        "# Ajout des arêtes entre les noeuds\n",
        "for u, v in G.edges():  # Pour chaque paire de noeuds\n",
        "    if G.nodes[u]['topic'] == G.nodes[v]['topic']: # Si les noeuds sont du même thème\n",
        "        G.add_edge(u, v) # Ajout de l'arête entre les noeuds\n",
        "\n",
        "# Affichage des noeuds les plus importants par thème\n",
        "top_n = 100\n",
        "for topic in range(n_topics): # Pour chaque thème\n",
        "    nodes = [node for node in G.nodes() if G.nodes[node]['topic'] == topic] # Récupération des noeuds du thème\n",
        "    nodes_sorted = sorted(nodes, key=lambda x: G.nodes[x]['prob'], reverse=True) # Tri des noeuds par probabilité\n",
        "    nodes_top = nodes_sorted[:top_n] # Récupération des noeuds les plus importants\n",
        "    for node in nodes_top: # Pour chaque noeud\n",
        "        G.nodes[node]['label'] = node.split()[-1] # Ajout du label correspondant au dernier mot du noeud\n",
        "\n",
        "# Affichage du graphe des thèmes\n",
        "pos = nx.spring_layout(G, k=0.5, iterations=50) # Positionnement des noeuds\n",
        "colors = plt.cm.get_cmap('hsv', n_topics) # Couleurs des noeuds\n",
        "for topic in range(n_topics): # Pour chaque thème\n",
        "    nodes = [node for node in G.nodes() if G.nodes[node]['topic'] == topic] # Récupération des noeuds du thème\n",
        "    nodes_top = [node for node in nodes if 'label' in G.nodes[node]] # Récupération des noeuds les plus importants\n",
        "    node_colors = [colors(topic) for _ in range(len(nodes_top))] # Couleurs des noeuds\n",
        "    nx.draw_networkx_nodes(G, pos, nodelist=nodes_top, node_color=node_colors, alpha=0.8) # Affichage des noeuds\n",
        "nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.5) # Affichage des arêtes \n",
        "node_labels = nx.get_node_attributes(G, 'label') # Récupération des labels des noeuds\n",
        "nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=10, font_family='sans-serif') # Affichage des labels\n",
        "plt.axis('off') # Suppression des axes\n",
        "plt.figure(figsize=(30, 20)) # Taille de la figure \n",
        "plt.show() # Affichage du graphe\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "III. L'appariement des textes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install scikit-learn # installation de scikit-learn\n",
        "!pip install -U sentence-transformers # installation de sentence-transformers "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer # import de sentence-transformers\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2') # chargement du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install faiss-cpu # installation de faiss-cpu\n",
        "import faiss # import de faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modèle des K-Means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouXR8d26aMi6"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans # import de KMeans\n",
        "\n",
        "# Définition du nombre de clusters à créer \n",
        "num_clusters = 63\n",
        "\n",
        "# Encodage des phrases en vecteurs \n",
        "phrase_embeddings = model.encode(matched_list)\n",
        "\n",
        "# Création du modèle KMeans \n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=0).fit(phrase_embeddings)\n",
        "\n",
        "# Calcul de la silhouette score \n",
        "silhouette_avg = silhouette_score(phrase_embeddings, kmeans.labels_)\n",
        "\n",
        "print(\"Le score silhouette vaut :\", silhouette_avg)\n",
        "\n",
        "# Création des clusters \n",
        "clusters = [[] for i in range(num_clusters)] # Clusters\n",
        "for i, label in enumerate(kmeans.labels_): # Pour chaque phrase et son cluster associé \n",
        "    clusters[label].append(matched_list[i]) # Ajout de la phrase au cluster correspondant\n",
        "\n",
        "# Affichage des clusters\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster {}:\".format(i+1))\n",
        "    print(clusters[i])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modèle Agglomerative Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import de AgglomerativeClustering\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Encodahe des phrases en vecteurs\n",
        "corpus_embeddings = model.encode(matched_list)\n",
        "\n",
        "# Normalisation des vecteurs\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Création du modèle AgglomerativeClustering\n",
        "clustering_model = AgglomerativeClustering(n_clusters=None, distance_threshold=1.5) \n",
        "# distance_threshold : distance maximale entre deux clusters pour qu'ils soient fusionnés\n",
        "clustering_model.fit(corpus_embeddings) # Evaluation du modele \n",
        "cluster_assignment = clustering_model.labels_ # Assignation des phrases aux clusters\n",
        "\n",
        "# Calcul de la silhouette score\n",
        "silhouette_avg = silhouette_score(corpus_embeddings, cluster_assignment)\n",
        "\n",
        "print(\"Le score silhouette vaut :\", silhouette_avg) # Affichage du score silhouette\n",
        "\n",
        "# Création des clusters\n",
        "clustered_sentences = {} # Dictionnaire des clusters\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment): # Pour chaque phrase et son cluster associé\n",
        "    if cluster_id not in clustered_sentences: # Si le cluster n'existe pas encore\n",
        "        clustered_sentences[cluster_id] = [] # Création du cluster\n",
        "    clustered_sentences[cluster_id].append(matched_list[sentence_id]) # Ajout de la phrase au cluster correspondant\n",
        "\n",
        "for i, cluster in clustered_sentences.items(): # Pour chaque cluster et ses phrases associées\n",
        "    print(\"Cluster \", i+1) # Affichage du numéro du cluster\n",
        "    print(cluster) # Affichage des phrases du cluster\n",
        "    print(\"\") # Saut de ligne"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modèle de DBScan sans FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN # import de DBSCAN\n",
        "\n",
        "# Encodage des phrases en vecteurs\n",
        "corpus_embeddings = model.encode(matched_list)\n",
        "\n",
        "# Normalisation des vecteurs\n",
        "corpus_embeddings = corpus_embeddings /  np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Création du modele DBSCAN\n",
        "clustering_model = DBSCAN(eps=0.5, min_samples=2, metric='euclidean')\n",
        "\n",
        "# Assignation des phrases aux clusters\n",
        "cluster_assignment = clustering_model.fit_predict(corpus_embeddings)\n",
        "\n",
        "# Calcul de la silhouette score\n",
        "silhouette_avg = silhouette_score(corpus_embeddings[indices[:, 1]], cluster_assignment)\n",
        "\n",
        "# Affichage du score silhouette\n",
        "print(\"Le score silhouette vaut :\", silhouette_avg)\n",
        "\n",
        "clustered_sentences = {} # Création des clusters\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment): # Pour chaque phrase et son cluster associé \n",
        "    if cluster_id not in clustered_sentences: # Si le cluster n'existe pas encore\n",
        "        clustered_sentences[cluster_id] = [] # Création du cluster\n",
        "    clustered_sentences[cluster_id].append(matched_list[sentence_id]) # Ajout de la phrase au cluster correspondant\n",
        "\n",
        "# Affichage des clusters\n",
        "for i, cluster in clustered_sentences.items():\n",
        "    print(\"Cluster \", i+1) # Affichage du numéro du cluster\n",
        "    print(cluster) # Affichage des phrases du cluster\n",
        "    print(\"\") # Saut de ligne\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modèle DBScan avec FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMmbWbtxa9fJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN # import de DBSCAN \n",
        "\n",
        "# Encodage des phrases en vecteurs \n",
        "corpus_embeddings = model.encode(matched_list)\n",
        "\n",
        "# Normalisation des vecteurs\n",
        "corpus_embeddings = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Création des index FAISS pour la recherche des plus proches voisins\n",
        "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])\n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "# Recherche des plus proches voisins \n",
        "_, indices = index.search(corpus_embeddings, k=5)  # 5 plus proches voisins\n",
        "\n",
        "# Création du modèle DBSCAN \n",
        "clustering_model = DBSCAN(eps=0.42, min_samples=5, metric='euclidean')\n",
        "\n",
        "#clustering_model = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_epsilon=0.40)\n",
        "\n",
        "# Assignation des phrases aux clusters \n",
        "cluster_assignment = clustering_model.fit_predict(corpus_embeddings[indices[:, 1]])\n",
        "\n",
        "# Calcul de la silhouette score\n",
        "silhouette_avg = silhouette_score(corpus_embeddings[indices[:, 1]], cluster_assignment)\n",
        "\n",
        "print(\"Le score silhouette vaut :\", silhouette_avg)\n",
        "\n",
        "# Création des clusters\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "    if cluster_id not in clustered_sentences: # Si le cluster n'existe pas encore\n",
        "        clustered_sentences[cluster_id] = [] # Création du cluster\n",
        "    clustered_sentences[cluster_id].append(matched_list[sentence_id]) # Ajout de la phrase au cluster correspondant\n",
        "\n",
        "# Affichage des clusters\n",
        "for i, cluster in clustered_sentences.items(): # Pour chaque cluster et ses phrases associées\n",
        "    print(\"Cluster \", i+1) # Affichage du numéro du cluster\n",
        "    print(cluster) # Affichage des phrases du cluster \n",
        "    print(\"\") # Saut de ligne\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Affichage graphique du modèle de DBScan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaQ4v7GdsQx4"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # import matplotlib\n",
        "from sklearn.decomposition import PCA # import de PCA\n",
        "\n",
        "# Encodage des phrases en vecteurs\n",
        "corpus_embeddings = model.encode(matched_list)\n",
        "\n",
        "# Normalisation des vecteurs \n",
        "corpus_embeddings = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Réglage de FAISS pour la recherche des plus proches voisins\n",
        "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])\n",
        "\n",
        "# Ajout des vecteurs au modèle FAISS \n",
        "index.add(corpus_embeddings)\n",
        "\n",
        "# Recherche des 5 plus proches voisins pour chaque phrase\n",
        "_, indices = index.search(corpus_embeddings, k=5)  # 5 plus proches voisins \n",
        "\n",
        "# Création du modèle DBSCAN\n",
        "clustering_model = DBSCAN(eps=1.5, min_samples=2, metric='euclidean')\n",
        "\n",
        "# Assignation des phrases aux clusters\n",
        "cluster_assignment = clustering_model.fit_predict(corpus_embeddings[indices[:, 1]])\n",
        "\n",
        "# Réduction de la dimension des vecteurs à 2\n",
        "pca = PCA(n_components=2)\n",
        "corpus_embeddings_2d = pca.fit_transform(corpus_embeddings) # Réduction de la dimension des vecteurs à 2\n",
        "\n",
        "# Affichage des clusters\n",
        "fig, ax = plt.subplots() # Création de la figure\n",
        "scatter = ax.scatter(corpus_embeddings_2d[:, 0], corpus_embeddings_2d[:, 1], c=cluster_assignment) # Affichage des points\n",
        "legend1 = ax.legend(*scatter.legend_elements(), \n",
        "                    loc=\"upper right\", title=\"Clusters\") # Affichage de la légende\n",
        "ax.add_artist(legend1) # Ajout de la légende à la figure\n",
        "plt.show() # Affichage de la figure\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modèle HDBScan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Pnv5WEbLEm"
      },
      "outputs": [],
      "source": [
        "!pip install hdbscan # installation de hdbscan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2-71x8UbJNg"
      },
      "outputs": [],
      "source": [
        "# Import des librairies nécessaires\n",
        "import hdbscan\n",
        "\n",
        "# Encodage des données en vecteurs\n",
        "corpus_embeddings = model.encode(matched_list)\n",
        "\n",
        "# Normalisation des vecteurs\n",
        "corpus_embeddings = corpus_embeddings / np.linalg.norm(corpus_embeddings, axis=1, keepdims=True)\n",
        "\n",
        "# Réglage de FAISS pour la recherche des plus proches voisins\n",
        "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])\n",
        "index.add(corpus_embeddings) # Ajout des vecteurs au modèle FAISS\n",
        "\n",
        "# Recherche des 5 plus proches voisins pour chaque phrase\n",
        "_, indices = index.search(corpus_embeddings, k=5)  # 5 plus proches voisins\n",
        "\n",
        "# Création du modèle HDBSCAN\n",
        "clustering_model = hdbscan.HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_epsilon=0.40)\n",
        "cluster_assignment = clustering_model.fit_predict(corpus_embeddings[indices[:, 1]]) # Assignation des phrases aux clusters\n",
        "\n",
        "# Calcul de la silhouette score\n",
        "silhouette_avg = silhouette_score(corpus_embeddings[indices[:, 1]], cluster_assignment)\n",
        "# Affichage du score silhouette\n",
        "print(\"Le score silhouette vaut :\", silhouette_avg)\n",
        "\n",
        "# Affichage des clusters\n",
        "clustered_sentences = {}\n",
        "for sentence_id, cluster_id in enumerate(cluster_assignment): # Pour chaque phrase et son cluster associé\n",
        "    if cluster_id not in clustered_sentences: # Si le cluster n'existe pas encore\n",
        "        clustered_sentences[cluster_id] = [] # Création du cluster\n",
        "    clustered_sentences[cluster_id].append(matched_list[sentence_id]) # Ajout de la phrase au cluster correspondant\n",
        "\n",
        "for i, cluster in clustered_sentences.items(): # Affichage des clusters\n",
        "    print(\"Cluster \", i+1) # Affichage du numéro du cluster\n",
        "    print(cluster) # Affichage des phrases du cluster\n",
        "    print(\"\") # Saut de ligne\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
