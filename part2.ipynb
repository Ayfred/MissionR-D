{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Téléchargement des packages Python/spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.42036581039429\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "import spacy\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier CSV dans un dataframe\n",
    "df = pd.read_csv('GooglePlayReviews.csv')\n",
    "\n",
    "# Fonction pour détecter la langue d'un commentaire\n",
    "def detect_language(comment):\n",
    "    try:\n",
    "        lang = detect(comment)\n",
    "        return lang\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Filtrer les commentaires en français\n",
    "df_fr = df[df['commentaire'].apply(lambda x: detect_language(x)=='fr')]\n",
    "\n",
    "df_fr.to_csv('Comments_Filtrés.csv', index=False, header = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture du fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the CSV file using the csv module\n",
    "with open('Comments_Filtrés.csv', newline='',  encoding='utf-8') as csvfile:\n",
    "    # create a csv reader object\n",
    "    reader = csv.reader(csvfile)\n",
    "    # create an empty list to store the strings\n",
    "    string_list = []\n",
    "    # loop through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # add the string to the list\n",
    "        #if detect(row[0]) == 'fr':\n",
    "        string_list.append(row[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple de texte\n",
    "text = \"Selon Wikipédia il l'appelle Laura 😊😊😊(hezignborzingiozrnoivznioznvionzo), La ponctuation est génial l’ensemble des signes qui, dans l’écrit, marquent les divisions et les liaisons des phrases et des membres 'de phrase'. Vous pouvez en apprendre plus sur \\\"la ponctuation\\\" en visitant ce site: https://www.larousse.fr/dictionnaires/francais/ponctuation/63717 parce que ce lien donne beaucoup d'informations intéressantes. 😊\"\n",
    "#text = \"Fonctionnalités, salut car beau temps aujourd'hui\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etapes du texte Preprocessing\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "listStopWords = [str(x) for x in nlp.Defaults.stop_words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove explanations : Bonjour, Avis aux Amateurs et Professionnels, Depuis une dizaine de jours, et quelques sorties Course à pied, je n'ai plus accès aux données \"Moyenne \" dans la synthèse proposé par l'application : plus d'allure moyenne, plus de rythme moyen, et plus de vitesse moyenne, etc. Avez une explication et/ou solution. Sinon très bonne application, ou il manque un mode fractionné. Très ergonomique et très intuitive. Merci 😁\n",
      "remove leopaul ' : Bonjour, Avis aux Amateurs et Professionnels, Depuis une dizaine de jours, et quelques sorties Course à pied, je n ai plus accès aux données \"Moyenne \" dans la synthèse proposé par l application : plus d allure moyenne, plus de rythme moyen, et plus de vitesse moyenne, etc. Avez une explication et/ou solution. Sinon très bonne application, ou il manque un mode fractionné. Très ergonomique et très intuitive. Merci 😁\n",
      "remove quotes : Bonjour, Avis aux Amateurs et Professionnels, Depuis une dizaine de jours, et quelques sorties Course pied, je ai plus accès aux données \"Moyenne dans la synthèse proposé par application plus allure moyenne, plus de rythme moyen, et plus de vitesse moyenne, etc. Avez une explication et/ou solution. Sinon très bonne application, ou il manque un mode fractionné. Très ergonomique et très intuitive. Merci \n",
      "remove links: Bonjour, Avis aux Amateurs et Professionnels, Depuis une dizaine de jours, et quelques sorties Course pied, je ai plus accès aux données \"Moyenne dans la synthèse proposé par application plus allure moyenne, plus de rythme moyen, et plus de vitesse moyenne, etc. Avez une explication et/ou solution. Sinon très bonne application, ou il manque un mode fractionné. Très ergonomique et très intuitive. Merci \n",
      "remove phone numbers and contacts: Bonjour  Avis Amateurs Professionnels  Depuis dizaine jours  quelques sorties Course pied  ai accès données  Moyenne la synthèse proposé application allure moyenne  rythme moyen  plus de vitesse moyenne   Avez explication solution  Sinon bonne application  ou il manque mode fractionné  Très ergonomique très intuitive  Merci \n",
      "remove pronoum and determinant : Bonjour   Avis Amateurs Professionnels   Depuis dizaine jours   sorties Course pied   ai accès données   Moyenne synthèse proposé application allure moyenne   rythme moyen   plus de vitesse moyenne    Avez explication solution   Sinon bonne application   ou manque mode fractionné   Très ergonomique très intuitive   Merci \n",
      "remove proper noun :       dizaine jours    sorties pied    ai accès données    synthèse proposé application allure moyenne    rythme moyen    plus de vitesse moyenne     explication solution    bonne application    ou manque mode fractionné    ergonomique très intuitive   \n",
      "      dizaine jours    sorties pied    ai accès données    synthèse proposé application allure moyenne    rythme moyen    plus de vitesse moyenne     explication solution    bonne application    ou manque mode fractionné    ergonomique très intuitive   \n"
     ]
    }
   ],
   "source": [
    "def united(text):\n",
    "    doc =  text.split(\" \");\n",
    "    # Itérer à travers chaque token dans le document\n",
    "    for token in doc:\n",
    "\n",
    "        #Remove explanations\n",
    "        # Si le token est \"parce que\", \"car\" ou \"puisque\", supprimer tous les tokens suivants dans la phrase\n",
    "        if token in [\"parce\", \"car\", \"puisque\"]:\n",
    "            doc = doc[:token.index()]\n",
    "            break\n",
    "    \n",
    "    # Supprimer tout ce qui est entre parenthèses\n",
    "    sentence = ' '.join(doc)\n",
    "    sentence = re.sub(r'\\([^()]*\\)', '', sentence)\n",
    "\n",
    "    print(f\"remove explanations : {sentence}\")\n",
    "\n",
    "    \n",
    "    #Removal of ' and the char that precede it\n",
    "    sentence = sentence.replace(\"'\", \" \")\n",
    "\n",
    "    print(f\"remove leopaul ' : {sentence}\")\n",
    "\n",
    "    words = sentence.split(\" \")\n",
    "    newSentence = \"\"\n",
    "    for word in words:\n",
    "        if len(word) > 1:\n",
    "            newSentence += word + \" \"\n",
    "    sentence = newSentence\n",
    "    \n",
    "    print(f\"remove quotes : {sentence}\")\n",
    "\n",
    "    sentence = re.sub('http[s]?://\\S+', '', sentence)\n",
    "\n",
    "    print(f\"remove links: {sentence}\")\n",
    "\n",
    "\n",
    "    # Regular expressions for phone numbers and email addresses\n",
    "    phone_regex = r\"(?<!\\d)(?:\\d[ -/\\\\_\\d]*){10}(?!\\d)\"\n",
    "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"  # matches john.doe@example.com\n",
    "\n",
    "    # Find phone numbers and email addresses in the sentence\n",
    "    sentence = re.sub(phone_regex,\"\", sentence)\n",
    "    sentence = re.sub(email_regex,\"\", sentence)\n",
    "    sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
    "    doc = sentence.split(\" \")\n",
    "    for word in doc:\n",
    "        if word in listStopWords:\n",
    "            doc.remove(word)\n",
    "    \n",
    "    sentence = ' '.join(doc)\n",
    "\n",
    "    print(f\"remove phone numbers and contacts: {sentence}\")\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    #texte = doc.text\n",
    "    text = \"\"\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"PRON\" or token.pos_ == \"DET\"):\n",
    "            continue\n",
    "        else:\n",
    "            if not token.text.__contains__(\"'\"):\n",
    "                text += token.text\n",
    "                text += \" \"\n",
    "    sentence = text\n",
    "\n",
    "    print(f\"remove pronoum and determinant : {sentence}\")\n",
    "\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if not (token.ent_type_ == \"ORG\" or token.ent_type_ == \"PERSON\" or token.text.istitle()):\n",
    "            result.append(token.text)\n",
    "    sentence =  \" \".join(result)\n",
    "    print(f\"remove proper noun : {sentence}\")\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\"\"\"\n",
    "for string in string_list:\n",
    "    united_string = united(string)\n",
    "    string_list_clean.append(united_string)\n",
    "print(string_list_clean)\n",
    "#run\n",
    "#print(united(text))\"\"\"\n",
    "united_string = united(string_list[0])\n",
    "print(united_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dizaine jours sorties pied ai accès données synthèse proposé application allure moyenne rythme moyen plus de vitesse moyenne explication solution bonne application ou manque mode fractionné ergonomique très intuitive \n"
     ]
    }
   ],
   "source": [
    "testttt = \"\"\n",
    "for token in nlp(united_string):\n",
    "    if (token.pos_ != \"SPACE\"):\n",
    "        testttt += token.text\n",
    "        testttt += \" \"\n",
    "print(testttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, Avis aux Amateurs et Professionnels, Depuis une dizaine de jours, et quelques sorties Course à pied, je n'ai plus accès aux données \"Moyenne \" dans la synthèse proposé par l'application : plus d'allure moyenne, plus de rythme moyen, et plus de vitesse moyenne, etc. Avez une explication et/ou solution. Sinon très bonne application, ou il manque un mode fractionné. Très ergonomique et très intuitive. Merci 😁\n"
     ]
    }
   ],
   "source": [
    "print(string_list[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                dizaine   jours      sorties     pied      n     accès   données   synthèse   proposé     application      d     allure   moyenne      plus   rythme   moyen      plus   de   vitesse   moyenne         explication      solution        bonne   application      manque   mode   fractionné        ergonomique   très   intuitive          '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "united_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat\n",
      "assoie\n",
      "sur\n",
      "canapé\n",
      "rouge\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of POS patterns to detect\n",
    "patterns = [\n",
    "    (\"noun noun\", [\"NOUN\", \"NOUN\"]),\n",
    "    (\"verb noun\", [\"VERB\", \"NOUN\"]),\n",
    "    (\"adjective noun\", [\"ADJ\", \"NOUN\"]),\n",
    "    (\"noun conjunction\", [\"NOUN\", \"CONJ\"]),\n",
    "    (\"noun adjective\", [\"NOUN\", \"ADJ\"]),\n",
    "    (\"noun noun noun\", [\"NOUN\", \"NOUN\", \"NOUN\"]),\n",
    "    (\"verb pronoun noun\", [\"VERB\", \"PRON\", \"NOUN\"]),\n",
    "    (\"verb noun noun\", [\"VERB\", \"NOUN\", \"NOUN\"]),\n",
    "    (\"verb adjective noun\", [\"VERB\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"adjective adjective noun\", [\"ADJ\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"noun preposition noun\", [\"NOUN\", \"ADP\", \"NOUN\"]),\n",
    "    (\"verb determiner noun\", [\"VERB\", \"DET\", \"NOUN\"]),\n",
    "    (\"verb noun preposition noun\", [\"VERB\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
    "    (\"adjective noun noun noun\", [\"ADJ\", \"NOUN\", \"NOUN\", \"NOUN\"]),\n",
    "    (\"adjective conjunction adjective\", [\"ADJ\", \"CONJ\", \"ADJ\"]),\n",
    "    (\"verb preposition adjective noun\", [\"VERB\", \"ADP\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"verb pronoun adjective noun\", [\"VERB\", \"PRON\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"noun conjunction noun noun\", [\"NOUN\", \"CONJ\", \"NOUN\", \"NOUN\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the reference patterns and check if there is a match in the sentence\n",
    "for pattern, pos_tags_pattern in patterns:\n",
    "    if len(pos_tags_pattern) != len(doc):\n",
    "        continue  # Skip patterns with different number of words\n",
    "    match = True\n",
    "    for i in range(len(doc)):\n",
    "        if pos_tags_pattern[i] != doc[i].pos_:\n",
    "            match = False\n",
    "            break\n",
    "    if match:\n",
    "        print(f\"Detected pattern '{pattern}' in sentence '{united_string}'\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input sentence\n",
    "sentence = \"The cat sat on the mat\"\n",
    "\n",
    "# Process the sentence with spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Iterate through the reference patterns and check if there is a match in the sentence\n",
    "for pattern, pos_tags_pattern in patterns:\n",
    "    if len(pos_tags_pattern) != len(doc):\n",
    "        continue  # Skip patterns with different number of words\n",
    "    match = True\n",
    "    for i in range(len(doc)):\n",
    "        if pos_tags_pattern[i] != doc[i].pos_:\n",
    "            match = False\n",
    "            break\n",
    "    if match:\n",
    "        print(f\"Detected pattern '{pattern}' in sentence '{sentence}'\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceByPattern = [[] for i in range((18))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"chat vert assoie sur canapé rouge\"\n",
    "doc = nlp(testttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      \n",
      "dizaine\n",
      "jours\n",
      "   \n",
      "sorties\n",
      "pied\n",
      "   \n",
      "ai\n",
      "accès\n",
      "données\n",
      "   \n",
      "synthèse\n",
      "proposé\n",
      "application\n",
      "allure\n",
      "moyenne\n",
      "   \n",
      "rythme\n",
      "moyen\n",
      "   \n",
      "plus\n",
      "de\n",
      "vitesse\n",
      "moyenne\n",
      "    \n",
      "explication\n",
      "solution\n",
      "   \n",
      "bonne\n",
      "application\n",
      "   \n",
      "ou\n",
      "manque\n",
      "mode\n",
      "fractionné\n",
      "   \n",
      "ergonomique\n",
      "très\n",
      "intuitive\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(doc) - 3):\n",
    "    if doc[i].pos_ == \"NOUN\":\n",
    "        if doc[i + 1].pos_ == \"CONJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            if doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[17].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                break\n",
    "            else:\n",
    "                sentenceByPattern[4].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"PREP\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[11].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "            break\n",
    "        elif doc[i + 1].pos_ == \"NOUN\":\n",
    "            if doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[6].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "            else:\n",
    "                sentenceByPattern[1].append(doc[i].text + \" \" + doc[i + 1].text)\n",
    "                break\n",
    "    elif doc[i].pos_ == \"VERB\":\n",
    "        if doc[i + 1].pos_ == \"PRON\":\n",
    "            if doc[i + 2].pos_ == \"ADJ\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[17].append(doc[i].text + \" \"+ doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                break\n",
    "            if doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[7].append(doc[i].text +\" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"PREP\" and doc[i + 2].pos_ == \"ADJ\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[16].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "        elif doc[i + 1].pos_ == \"NOUN\" :\n",
    "            if doc[i + 2].pos_ == \"PREP\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[13].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                break\n",
    "            elif doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[8].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "            else:\n",
    "                sentenceByPattern[2].append(doc[i].text + \" \" + doc[i + 1].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"DET\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[12].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "            break\n",
    "        elif doc[i + 1].pos_ == \"ADJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[9].append(doc[i].text + \" \" + doc[i + 1].text + \" \" +doc[i + 2].text)\n",
    "            break\n",
    "    elif doc[i].pos_ == \"ADJ\" :\n",
    "        if doc[i + 1].pos_ == \"CONJ\" and doc[i + 2].pos_ == \"ADJ\":\n",
    "            sentenceByPattern[15].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "            break\n",
    "        elif doc[i + 1].pos_ == \"NOUN\":\n",
    "            if doc[i + 2].pos_ == \"NOUN\":\n",
    "                if doc[i + 3].pos_ == \"NOUN\":\n",
    "                    sentenceByPattern[14].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                    break\n",
    "                else:\n",
    "                    sentenceByPattern[5].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                    break\n",
    "            else:\n",
    "                sentenceByPattern[3].append(doc[i].text + \" \" + doc[i + 1].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"ADJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[10].append(doc[i].text + \" \" + doc[i + 1].text +\" \" + doc[i + 2].text)\n",
    "            break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['dizaine jours'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceByPattern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
