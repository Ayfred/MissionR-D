{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Téléchargement des packages Python/spaCy\n",
    "#!python -m spacy download fr_core_news_sm\n",
    "import time\n",
    "start = time.time()\n",
    "#Importation des packages Python/spaCy\n",
    "import spacy\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                dizaine   jours      sorties     pied      n     accès   données   synthèse   proposé     application      d     allure   moyenne      plus   rythme   moyen      plus   de   vitesse   moyenne         explication      solution        bonne   application      manque   mode   fractionné        ergonomique   très   intuitive          \n"
     ]
    }
   ],
   "source": [
    "# open the CSV file using the csv module\n",
    "with open('GooglePlayComments.csv', newline='',  encoding='utf-8') as csvfile:\n",
    "    # create a csv reader object\n",
    "    reader = csv.reader(csvfile)\n",
    "    # create an empty list to store the strings\n",
    "    string_list = []\n",
    "    # loop through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # add the string to the list\n",
    "        #print(row)\n",
    "        #if detect(row[0]) == 'fr':\n",
    "        string_list.append(row[0])\n",
    "\n",
    "#print(len(string_list))\n",
    "\n",
    "#print the list of strings\n",
    "#print(string_list)\n",
    "\n",
    "string_list_clean = []\n",
    "\n",
    "\n",
    "#Exemple de texte\n",
    "text = \"Selon Wikipédia il l'appelle Laura 😊😊😊(hezignborzingiozrnoivznioznvionzo), La ponctuation est génial l’ensemble des signes qui, dans l’écrit, marquent les divisions et les liaisons des phrases et des membres 'de phrase'. Vous pouvez en apprendre plus sur \\\"la ponctuation\\\" en visitant ce site: https://www.larousse.fr/dictionnaires/francais/ponctuation/63717 parce que ce lien donne beaucoup d'informations intéressantes. 😊\"\n",
    "#text = \"Fonctionnalités, salut car beau temps aujourd'hui\"\n",
    "\n",
    "#Etapes du texte Preprocessing\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "listStopWords = [str(x) for x in nlp.Defaults.stop_words]\n",
    "\n",
    "#Tokenisation\n",
    "def tokenisation(text):\n",
    "    return nlp(text)\n",
    "\n",
    "#Affichage des tokens\n",
    "def affichage_tokens(doc):\n",
    "    for token in doc:\n",
    "        # Print the text and the predicted part-of-speech tag\n",
    "        print(token.text, token.pos_)\n",
    "\n",
    "#Removal of explanations\n",
    "def explanationsRemoval(doc):\n",
    "    sentence = doc.text\n",
    "    # Itérer à travers chaque token dans le document\n",
    "    for token in doc:\n",
    "        # Si le token est \"parce que\", \"car\" ou \"puisque\", supprimer tous les tokens suivants dans la phrase\n",
    "        if token.text.lower() in [\"parce\", \"car\", \"puisque\"]:\n",
    "            sentence = sentence[:token.idx]\n",
    "            break\n",
    "    # Supprimer tout ce qui est entre parenthèses\n",
    "    sentence = re.sub(r'\\([^()]*\\)', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "#Removal of quotes\n",
    "def quotesRemoval(doc):\n",
    "    filtered_text = \"\"\n",
    "    Punc_list = ['\"', \"'\"]\n",
    "    boolean_punct = False\n",
    "    for token in doc:\n",
    "        if not token.text in Punc_list:\n",
    "            if not boolean_punct:\n",
    "                filtered_text += token.text\n",
    "                filtered_text += \" \"\n",
    "        else :\n",
    "            if boolean_punct :\n",
    "                boolean_punct = False\n",
    "            else :\n",
    "                boolean_punct = True\n",
    "    return filtered_text\n",
    "\n",
    "#Removal of URLs\n",
    "def urlRemoval(text):\n",
    "    return re.sub('http[s]?://\\S+', '', text)\n",
    "\n",
    "#Removal of contact information\n",
    "def contactRemoval(text):\n",
    "    # Regular expressions for phone numbers and email addresses\n",
    "    phone_regex = r\"(?<!\\d)(?:\\d[ -/\\\\_\\d]*){10}(?!\\d)\"\n",
    "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"  # matches john.doe@example.com\n",
    "\n",
    "    # Find phone numbers and email addresses in the sentence\n",
    "    text = re.sub(phone_regex,\"\", text)\n",
    "    text = re.sub(email_regex,\"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "#Removal of symbols\n",
    "def symbolsRemoval(text):\n",
    "    return re.sub(r'[^\\w]', ' ', text)\n",
    "\n",
    "#Removal of stop words\n",
    "def stopWordsRemoval(text):\n",
    "    splitext = text.split(\" \")\n",
    "    for word in splitext:\n",
    "        if word in listStopWords:\n",
    "            splitext.remove(word)\n",
    "    return ' '.join(splitext)\n",
    "\n",
    "def pronounAndDetRemoval(doc):\n",
    "    #texte = doc.text\n",
    "    sentence = \"\"\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"PRON\" or token.pos_ == \"DET\"):\n",
    "            continue\n",
    "        else:\n",
    "            sentence += token.text_with_ws\n",
    "            sentence += \" \"\n",
    "    return sentence\n",
    "\n",
    "def propNounRemoval(doc):\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if not (token.ent_type_ == \"ORG\" or token.ent_type_ == \"PERSON\" or token.text.istitle()):\n",
    "            result.append(token.text)\n",
    "    return \" \".join(result)\n",
    "\n",
    "def united(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sentence = doc.text\n",
    "    # Itérer à travers chaque token dans le document\n",
    "    for token in doc:\n",
    "\n",
    "        #Remove explanations\n",
    "        # Si le token est \"parce que\", \"car\" ou \"puisque\", supprimer tous les tokens suivants dans la phrase\n",
    "        if token.text.lower() in [\"parce\", \"car\", \"puisque\"]:\n",
    "            sentence = sentence[:token.idx]\n",
    "            break\n",
    "    # Supprimer tout ce qui est entre parenthèses\n",
    "    sentence = re.sub(r'\\([^()]*\\)', '', sentence)\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    #Filter\n",
    "    filtered_text = \"\"\n",
    "    Punc_list = ['\"', \"'\"]\n",
    "    boolean_punct = False\n",
    "    for token in doc:\n",
    "        if not token.text in Punc_list:\n",
    "            if not boolean_punct:\n",
    "                filtered_text += token.text\n",
    "                filtered_text += \" \"\n",
    "        else :\n",
    "            if boolean_punct :\n",
    "                boolean_punct = False\n",
    "            else :\n",
    "                boolean_punct = True\n",
    "\n",
    "    sentence = filtered_text\n",
    "    sentence = re.sub('http[s]?://\\S+', '', sentence)\n",
    "\n",
    "    # Regular expressions for phone numbers and email addresses\n",
    "    phone_regex = r\"(?<!\\d)(?:\\d[ -/\\\\_\\d]*){10}(?!\\d)\"\n",
    "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"  # matches john.doe@example.com\n",
    "\n",
    "    # Find phone numbers and email addresses in the sentence\n",
    "    sentence = re.sub(phone_regex,\"\", sentence)\n",
    "    sentence = re.sub(email_regex,\"\", sentence)\n",
    "    sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
    "    splitext = sentence.split(\" \")\n",
    "    for word in splitext:\n",
    "        if word in listStopWords:\n",
    "            splitext.remove(word)\n",
    "    sentence= ' '.join(splitext)\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    #texte = doc.text\n",
    "    text = \"\"\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"PRON\" or token.pos_ == \"DET\"):\n",
    "            continue\n",
    "        else:\n",
    "            text += token.text_with_ws\n",
    "            text += \" \"\n",
    "    sentence = text\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if not (token.ent_type_ == \"ORG\" or token.ent_type_ == \"PERSON\" or token.text.istitle()):\n",
    "            result.append(token.text)\n",
    "    sentence =  \" \".join(result)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\"\"\"\n",
    "for string in string_list:\n",
    "    united_string = united(string)\n",
    "    string_list_clean.append(united_string)\n",
    "print(string_list_clean)\n",
    "#run\n",
    "#print(united(text))\"\"\"\n",
    "united_string = united(string_list[0])\n",
    "print(united_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'ja', 'score': 0.9999999999991745}\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "nlp.add_pipe('language_detector', last=True)\n",
    "text = 'はいあぃがとう.'\n",
    "doc = nlp(text)\n",
    "print(doc._.language)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                dizaine   jours      sorties     pied      n     accès   données   synthèse   proposé     application      d     allure   moyenne      plus   rythme   moyen      plus   de   vitesse   moyenne         explication      solution        bonne   application      manque   mode   fractionné        ergonomique   très   intuitive          '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "united_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'language': 'fr', 'score': 0.9999953794949574}\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(united_string)\n",
    "print(doc._.language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                \n",
      "dizaine\n",
      "  \n",
      "jours\n",
      "     \n",
      "sorties\n",
      "    \n",
      "pied\n",
      "     \n",
      "n\n",
      "    \n",
      "accès\n",
      "  \n",
      "données\n",
      "  \n",
      "synthèse\n",
      "  \n",
      "proposé\n",
      "    \n",
      "application\n",
      "     \n",
      "d\n",
      "    \n",
      "allure\n",
      "  \n",
      "moyenne\n",
      "     \n",
      "plus\n",
      "  \n",
      "rythme\n",
      "  \n",
      "moyen\n",
      "     \n",
      "plus\n",
      "  \n",
      "de\n",
      "  \n",
      "vitesse\n",
      "  \n",
      "moyenne\n",
      "        \n",
      "explication\n",
      "     \n",
      "solution\n",
      "       \n",
      "bonne\n",
      "  \n",
      "application\n",
      "     \n",
      "manque\n",
      "  \n",
      "mode\n",
      "  \n",
      "fractionné\n",
      "       \n",
      "ergonomique\n",
      "  \n",
      "très\n",
      "  \n",
      "intuitive\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of POS patterns to detect\n",
    "patterns = [\n",
    "    (\"noun noun\", [\"NOUN\", \"NOUN\"]),\n",
    "    (\"verb noun\", [\"VERB\", \"NOUN\"]),\n",
    "    (\"adjective noun\", [\"ADJ\", \"NOUN\"]),\n",
    "    (\"noun conjunction\", [\"NOUN\", \"CONJ\"]),\n",
    "    (\"noun adjective\", [\"NOUN\", \"ADJ\"]),\n",
    "    (\"noun noun noun\", [\"NOUN\", \"NOUN\", \"NOUN\"]),\n",
    "    (\"verb pronoun noun\", [\"VERB\", \"PRON\", \"NOUN\"]),\n",
    "    (\"verb noun noun\", [\"VERB\", \"NOUN\", \"NOUN\"]),\n",
    "    (\"verb adjective noun\", [\"VERB\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"adjective adjective noun\", [\"ADJ\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"noun preposition noun\", [\"NOUN\", \"ADP\", \"NOUN\"]),\n",
    "    (\"verb determiner noun\", [\"VERB\", \"DET\", \"NOUN\"]),\n",
    "    (\"verb noun preposition noun\", [\"VERB\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
    "    (\"adjective noun noun noun\", [\"ADJ\", \"NOUN\", \"NOUN\", \"NOUN\"]),\n",
    "    (\"adjective conjunction adjective\", [\"ADJ\", \"CONJ\", \"ADJ\"]),\n",
    "    (\"verb preposition adjective noun\", [\"VERB\", \"ADP\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"verb pronoun adjective noun\", [\"VERB\", \"PRON\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"noun conjunction noun noun\", [\"NOUN\", \"CONJ\", \"NOUN\", \"NOUN\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the reference patterns and check if there is a match in the sentence\n",
    "for pattern, pos_tags_pattern in patterns:\n",
    "    if len(pos_tags_pattern) != len(doc):\n",
    "        continue  # Skip patterns with different number of words\n",
    "    match = True\n",
    "    for i in range(len(doc)):\n",
    "        if pos_tags_pattern[i] != doc[i].pos_:\n",
    "            match = False\n",
    "            break\n",
    "    if match:\n",
    "        print(f\"Detected pattern '{pattern}' in sentence '{united_string}'\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input sentence\n",
    "sentence = \"The cat sat on the mat\"\n",
    "\n",
    "# Process the sentence with spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Iterate through the reference patterns and check if there is a match in the sentence\n",
    "for pattern, pos_tags_pattern in patterns:\n",
    "    if len(pos_tags_pattern) != len(doc):\n",
    "        continue  # Skip patterns with different number of words\n",
    "    match = True\n",
    "    for i in range(len(doc)):\n",
    "        if pos_tags_pattern[i] != doc[i].pos_:\n",
    "            match = False\n",
    "            break\n",
    "    if match:\n",
    "        print(f\"Detected pattern '{pattern}' in sentence '{sentence}'\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceByPattern = [[] for i in range((18))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"cat dog sat on mat\"\n",
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(doc) - 3):\n",
    "    if doc[i].pos_ == \"NOUN\":\n",
    "        if doc[i + 1].pos_ == \"CONJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            if doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[17].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                break\n",
    "            else:\n",
    "                sentenceByPattern[4].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"PREP\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[11].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "            break\n",
    "        elif doc[i + 1].pos_ == \"NOUN\":\n",
    "            if doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[6].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "            else:\n",
    "                sentenceByPattern[1].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "    elif doc[i].pos_ == \"VERB\":\n",
    "        if doc[i + 1].pos_ == \"PRON\":\n",
    "            if doc[i + 2].pos_ == \"ADJ\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[17].append(doc[i].text + doc[i + 1].text + doc[i + 2].text + doc[i + 3].text)\n",
    "                break\n",
    "            if doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[7].append(doc[i].text + doc[i + 1].text + doc[i + 2].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"PREP\" and doc[i + 2].pos_ == \"ADJ\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[16].append(doc[i].text + doc[i + 1].text + doc[i + 2].text + doc[i + 3].text)\n",
    "        elif doc[i + 1].pos_ == \"NOUN\" :\n",
    "            if doc[i + 2].pos_ == \"PREP\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[13].append(doc[i].text + doc[i + 1].text + doc[i + 2].text + doc[i + 3].text)\n",
    "                break\n",
    "            elif doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[8].append(doc[i].text + doc[i + 1].text + doc[i + 2].text)\n",
    "                break\n",
    "            else:\n",
    "                sentenceByPattern[2].append(doc[i].text + doc[i + 1].text + doc[i + 2].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"DET\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[12].append(doc[i].text + doc[i + 1].text + doc[i + 2].text)\n",
    "            break\n",
    "        elif doc[i + 1].pos_ == \"ADJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[9].append(doc[i].text + doc[i + 1].text + doc[i + 2].text)\n",
    "            break\n",
    "    elif doc[i].pos_ == \"ADJ\" :\n",
    "        if doc[i + 1].pos_ == \"CONJ\" and doc[i + 2].pos_ == \"ADJ\":\n",
    "            sentenceByPattern[15].append(doc[i].text + doc[i + 1].text + doc[i + 2].text)\n",
    "            break\n",
    "        elif doc[i + 1].pos_ == \"NOUN\":\n",
    "            if doc[i + 2].pos_ == \"NOUN\":\n",
    "                if doc[i + 3].pos_ == \"NOUN\":\n",
    "                    sentenceByPattern[14].append(doc[i].text + doc[i + 1].text + doc[i + 2].text + doc[i + 3].text)\n",
    "                    break\n",
    "                else:\n",
    "                    sentenceByPattern[5].append(doc[i].text + doc[i + 1].text + doc[i + 2].text)\n",
    "                    break\n",
    "            else:\n",
    "                sentenceByPattern[3].append(doc[i].text + doc[i + 1].text + doc[i + 2].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"ADJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[10].append(doc[i].text + doc[i + 1].text + doc[i + 2].text)\n",
    "            break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " ['dogsaton'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceByPattern"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
