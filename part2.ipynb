{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#T√©l√©chargement des packages Python/spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download fr_core_news_sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.65170979499817\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "import spacy\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le fichier CSV dans un dataframe\n",
    "df = pd.read_csv('GooglePlayReviews.csv')\n",
    "\n",
    "# Fonction pour d√©tecter la langue d'un commentaire\n",
    "def detect_language(comment):\n",
    "    try:\n",
    "        lang = detect(comment)\n",
    "        return lang\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Filtrer les commentaires en fran√ßais\n",
    "df_fr = df[df['commentaire'].apply(lambda x: detect_language(x)=='fr')]\n",
    "\n",
    "df_fr.to_csv('Comments_Filtr√©s.csv', index=False, header = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture du fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the CSV file using the csv module\n",
    "with open('Comments_Filtr√©s.csv', newline='',  encoding='utf-8') as csvfile:\n",
    "    # create a csv reader object\n",
    "    reader = csv.reader(csvfile)\n",
    "    # create an empty list to store the strings\n",
    "    string_list = []\n",
    "    # loop through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # add the string to the list\n",
    "        #if detect(row[0]) == 'fr':\n",
    "        string_list.append(row[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple de textes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple de texte\n",
    "text = \"Selon Wikip√©dia il l'appelle Laura üòäüòäüòä(hezignborzingiozrnoivznioznvionzo), La ponctuation est g√©nial l‚Äôensemble des signes qui, dans l‚Äô√©crit, marquent les divisions et les liaisons des phrases et des membres 'de phrase'. Vous pouvez en apprendre plus sur \\\"la ponctuation\\\" en visitant ce site: https://www.larousse.fr/dictionnaires/francais/ponctuation/63717 parce que ce lien donne beaucoup d'informations int√©ressantes. üòä\"\n",
    "#text = \"Fonctionnalit√©s, salut car beau temps aujourd'hui\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etapes du texte Preprocessing\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "listStopWords = [str(x) for x in nlp.Defaults.stop_words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove explanations : Bonjour, Avis aux Amateurs et Professionnels, Depuis une dizaine de jours, et quelques sorties Course √† pied, je n'ai plus acc√®s aux donn√©es \"Moyenne \" dans la synth√®se propos√© par l'application : plus d'allure moyenne, plus de rythme moyen, et plus de vitesse moyenne, etc. Avez une explication et/ou solution. Sinon tr√®s bonne application, ou il manque un mode fractionn√©. Tr√®s ergonomique et tr√®s intuitive. Merci üòÅ\n",
      "remove links: Bonjour, Avis aux Amateurs et Professionnels, Depuis une dizaine de jours, et quelques sorties Course √† pied, je n'ai plus acc√®s aux donn√©es \"Moyenne \" dans la synth√®se propos√© par l'application : plus d'allure moyenne, plus de rythme moyen, et plus de vitesse moyenne, etc. Avez une explication et/ou solution. Sinon tr√®s bonne application, ou il manque un mode fractionn√©. Tr√®s ergonomique et tr√®s intuitive. Merci üòÅ\n",
      "remove links2: Bonjour, Avis aux Amateurs et Professionnels, Depuis une dizaine de jours, et quelques sorties Course √† pied, je n'ai plus acc√®s aux donn√©es  dans la synth√®se propos√© par l'application : plus d'allure moyenne, plus de rythme moyen, et plus de vitesse moyenne, etc. Avez une explication et/ou solution. Sinon tr√®s bonne application, ou il manque un mode fractionn√©. Tr√®s ergonomique et tr√®s intuitive. Merci üòÅ\n",
      "remove phone numbers and contacts: Bonjour  Avis Amateurs Professionnels  Depuis dizaine jours  quelques sorties Course pied  n acc√®s donn√©es  la synth√®se propos√© l application   d allure moyenne  plus rythme moyen  plus de vitesse moyenne   Avez explication solution  Sinon bonne application  ou il manque mode fractionn√©  Tr√®s ergonomique tr√®s intuitive  Merci  \n",
      "remove leopaul: Bonjour  Avis Amateurs Professionnels  Depuis dizaine jours  quelques sorties Course pied  n acc√®s donn√©es  la synth√®se propos√© l application   d allure moyenne  plus rythme moyen  plus de vitesse moyenne   Avez explication solution  Sinon bonne application  ou il manque mode fractionn√©  Tr√®s ergonomique tr√®s intuitive  Merci  \n",
      "remove quotes : Bonjour Avis Amateurs Professionnels Depuis dizaine jours quelques sorties Course pied acc√®s donn√©es la synth√®se propos√© application allure moyenne plus rythme moyen plus de vitesse moyenne Avez explication solution Sinon bonne application ou il manque mode fractionn√© Tr√®s ergonomique tr√®s intuitive Merci \n",
      "remove pronoum and determinant : Bonjour Avis Amateurs Professionnels Depuis dizaine jours sorties Course pied acc√®s donn√©es synth√®se propos√© application allure moyenne plus rythme moyen plus de vitesse moyenne Avez explication solution Sinon bonne application ou manque mode fractionn√© Tr√®s ergonomique tr√®s intuitive Merci \n",
      "remove proper noun : dizaine jours sorties pied acc√®s donn√©es synth√®se propos√© application allure moyenne plus rythme moyen plus de vitesse moyenne explication solution bonne application ou manque mode fractionn√© ergonomique tr√®s intuitive\n",
      "dizaine jours sorties pied acc√®s donn√©es synth√®se propos√© application allure moyenne plus rythme moyen plus de vitesse moyenne explication solution bonne application ou manque mode fractionn√© ergonomique tr√®s intuitive\n"
     ]
    }
   ],
   "source": [
    "def united(text):\n",
    "    doc =  text.split(\" \");\n",
    "    # It√©rer √† travers chaque token dans le document\n",
    "    for token in doc:\n",
    "\n",
    "        #Remove explanations\n",
    "        # Si le token est \"parce que\", \"car\" ou \"puisque\", supprimer tous les tokens suivants dans la phrase\n",
    "        if token in [\"parce\", \"car\", \"puisque\"]:\n",
    "            doc = doc[:token.index()]\n",
    "            break\n",
    "    \n",
    "    # Supprimer tout ce qui est entre parenth√®ses\n",
    "    sentence = ' '.join(doc)\n",
    "    sentence = re.sub(r'\\([^()]*\\)', '', sentence)\n",
    "\n",
    "    print(f\"remove explanations : {sentence}\")\n",
    "\n",
    "    sentence = re.sub('http[s]?://\\S+', '', sentence)\n",
    "\n",
    "    print(f\"remove links: {sentence}\")\n",
    "\n",
    "\n",
    "    # Regular expressions for phone numbers and email addresses\n",
    "    quotes = r'\\\"[^\\\"]+\\\"'\n",
    "    phone_regex = r\"(?<!\\d)(?:\\d[ -/\\\\_\\d]*){10}(?!\\d)\"\n",
    "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"  # matches john.doe@example.com\n",
    "\n",
    "    # Find phone numbers and email addresses in the sentence\n",
    "    sentence = re.sub(quotes,\"\", sentence)\n",
    "\n",
    "    print(f\"remove links2: {sentence}\")\n",
    "\n",
    "\n",
    "    sentence = re.sub(phone_regex,\"\", sentence)\n",
    "    sentence = re.sub(email_regex,\"\", sentence)\n",
    "    sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
    "    doc = sentence.split(\" \")\n",
    "    for word in doc:\n",
    "        if word in listStopWords:\n",
    "            doc.remove(word)\n",
    "    \n",
    "    sentence = ' '.join(doc)\n",
    "\n",
    "    print(f\"remove phone numbers and contacts: {sentence}\")\n",
    "\n",
    "    #Removal of ' and the char that precede it\n",
    "    sentence = sentence.replace(\"'\", \" \")\n",
    "\n",
    "    print(f\"remove leopaul: {sentence}\")\n",
    "\n",
    "    words = sentence.split(\" \")\n",
    "    newSentence = \"\"\n",
    "    for word in words:\n",
    "        if len(word) > 1:\n",
    "            newSentence += word + \" \"\n",
    "    sentence = newSentence\n",
    "    \n",
    "    print(f\"remove quotes : {sentence}\")\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    #texte = doc.text\n",
    "    text = \"\"\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"PRON\" or token.pos_ == \"DET\"):\n",
    "            continue\n",
    "        else:\n",
    "            if not token.text.__contains__(\"'\"):\n",
    "                text += token.text\n",
    "                text += \" \"\n",
    "    sentence = text\n",
    "\n",
    "    print(f\"remove pronoum and determinant : {sentence}\")\n",
    "\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if not (token.ent_type_ == \"ORG\" or token.ent_type_ == \"PERSON\" or token.text.istitle()):\n",
    "            result.append(token.text)\n",
    "    sentence =  \" \".join(result)\n",
    "    print(f\"remove proper noun : {sentence}\")\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\"\"\"\n",
    "for string in string_list:\n",
    "    united_string = united(string)\n",
    "    string_list_clean.append(united_string)\n",
    "print(string_list_clean)\n",
    "#run\n",
    "#print(united(text))\"\"\"\n",
    "united_string = united(string_list[0])\n",
    "print(united_string)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dizaine jours sorties pied acc√®s donn√©es synth√®se propos√© application allure moyenne plus rythme moyen plus de vitesse moyenne explication solution bonne application ou manque mode fractionn√© ergonomique tr√®s intuitive'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "united_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of POS patterns to detect\n",
    "patterns = [\n",
    "    (\"noun noun\", [\"NOUN\", \"NOUN\"]),\n",
    "    (\"verb noun\", [\"VERB\", \"NOUN\"]),\n",
    "    (\"adjective noun\", [\"ADJ\", \"NOUN\"]),\n",
    "    (\"noun conjunction\", [\"NOUN\", \"CONJ\"]),\n",
    "    (\"noun adjective\", [\"NOUN\", \"ADJ\"]),\n",
    "    (\"noun noun noun\", [\"NOUN\", \"NOUN\", \"NOUN\"]),\n",
    "    (\"verb pronoun noun\", [\"VERB\", \"PRON\", \"NOUN\"]),\n",
    "    (\"verb noun noun\", [\"VERB\", \"NOUN\", \"NOUN\"]),\n",
    "    (\"verb adjective noun\", [\"VERB\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"adjective adjective noun\", [\"ADJ\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"noun preposition noun\", [\"NOUN\", \"ADP\", \"NOUN\"]),\n",
    "    (\"verb determiner noun\", [\"VERB\", \"DET\", \"NOUN\"]),\n",
    "    (\"verb noun preposition noun\", [\"VERB\", \"NOUN\", \"ADP\", \"NOUN\"]),\n",
    "    (\"adjective noun noun noun\", [\"ADJ\", \"NOUN\", \"NOUN\", \"NOUN\"]),\n",
    "    (\"adjective conjunction adjective\", [\"ADJ\", \"CONJ\", \"ADJ\"]),\n",
    "    (\"verb preposition adjective noun\", [\"VERB\", \"ADP\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"verb pronoun adjective noun\", [\"VERB\", \"PRON\", \"ADJ\", \"NOUN\"]),\n",
    "    (\"noun conjunction noun noun\", [\"NOUN\", \"CONJ\", \"NOUN\", \"NOUN\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Iterate through the reference patterns and check if there is a match in the sentence\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m pattern, pos_tags_pattern \u001b[39min\u001b[39;00m patterns:\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(pos_tags_pattern) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(doc):\n\u001b[0;32m      4\u001b[0m         \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Skip patterns with different number of words\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     match \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "# Iterate through the reference patterns and check if there is a match in the sentence\n",
    "for pattern, pos_tags_pattern in patterns:\n",
    "    if len(pos_tags_pattern) != len(doc):\n",
    "        continue  # Skip patterns with different number of words\n",
    "    match = True\n",
    "    for i in range(len(doc)):\n",
    "        if pos_tags_pattern[i] != doc[i].pos_:\n",
    "            match = False\n",
    "            break\n",
    "    if match:\n",
    "        print(f\"Detected pattern '{pattern}' in sentence '{united_string}'\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input sentence\n",
    "sentence = \"The cat sat on the mat\"\n",
    "\n",
    "# Process the sentence with spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Iterate through the reference patterns and check if there is a match in the sentence\n",
    "for pattern, pos_tags_pattern in patterns:\n",
    "    if len(pos_tags_pattern) != len(doc):\n",
    "        continue  # Skip patterns with different number of words\n",
    "    match = True\n",
    "    for i in range(len(doc)):\n",
    "        if pos_tags_pattern[i] != doc[i].pos_:\n",
    "            match = False\n",
    "            break\n",
    "    if match:\n",
    "        print(f\"Detected pattern '{pattern}' in sentence '{sentence}'\")\n",
    "        break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceByPattern = [[] for i in range((18))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"chat vert assoie sur canap√© rouge\"\n",
    "doc = nlp(united_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply SAFE POS patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(doc) - 3):\n",
    "    if doc[i].pos_ == \"NOUN\":\n",
    "        if doc[i + 1].pos_ == \"CONJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            if doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[17].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                break\n",
    "            else:\n",
    "                sentenceByPattern[4].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"PREP\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[11].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "            break\n",
    "        elif doc[i + 1].pos_ == \"NOUN\":\n",
    "            if doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[6].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "            else:\n",
    "                sentenceByPattern[1].append(doc[i].text + \" \" + doc[i + 1].text)\n",
    "                break\n",
    "    elif doc[i].pos_ == \"VERB\":\n",
    "        if doc[i + 1].pos_ == \"PRON\":\n",
    "            if doc[i + 2].pos_ == \"ADJ\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[17].append(doc[i].text + \" \"+ doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                break\n",
    "            if doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[7].append(doc[i].text +\" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"PREP\" and doc[i + 2].pos_ == \"ADJ\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[16].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "        elif doc[i + 1].pos_ == \"NOUN\" :\n",
    "            if doc[i + 2].pos_ == \"PREP\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[13].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                break\n",
    "            elif doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[8].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                break\n",
    "            else:\n",
    "                sentenceByPattern[2].append(doc[i].text + \" \" + doc[i + 1].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"DET\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[12].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "            break\n",
    "        elif doc[i + 1].pos_ == \"ADJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[9].append(doc[i].text + \" \" + doc[i + 1].text + \" \" +doc[i + 2].text)\n",
    "            break\n",
    "    elif doc[i].pos_ == \"ADJ\" :\n",
    "        if doc[i + 1].pos_ == \"CONJ\" and doc[i + 2].pos_ == \"ADJ\":\n",
    "            sentenceByPattern[15].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "            break\n",
    "        elif doc[i + 1].pos_ == \"NOUN\":\n",
    "            if doc[i + 2].pos_ == \"NOUN\":\n",
    "                if doc[i + 3].pos_ == \"NOUN\":\n",
    "                    sentenceByPattern[14].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                    break\n",
    "                else:\n",
    "                    sentenceByPattern[5].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                    break\n",
    "            else:\n",
    "                sentenceByPattern[3].append(doc[i].text + \" \" + doc[i + 1].text)\n",
    "                break\n",
    "        elif doc[i + 1].pos_ == \"ADJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "            sentenceByPattern[10].append(doc[i].text + \" \" + doc[i + 1].text +\" \" + doc[i + 2].text)\n",
    "            break\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['dizaine jours', 'dizaine jours'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceByPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dizaine jours sorties pied acc√®s donn√©es synth√®se propos√© application allure moyenne plus rythme moyen plus de vitesse moyenne explication solution bonne application ou manque mode fractionn√© ergonomique tr√®s intuitive'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "united_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPattern(doc):\n",
    "    if(len(doc) < 4):\n",
    "        return None\n",
    "    for i in range(len(doc) - 3):\n",
    "        if doc[i].pos_ == \"NOUN\":\n",
    "            if doc[i + 1].pos_ == \"CONJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "                if doc[i + 3].pos_ == \"NOUN\":\n",
    "                    sentenceByPattern[17].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                    return getPattern(doc[i + 4:])\n",
    "                else:\n",
    "                    sentenceByPattern[4].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                    return getPattern(doc[i + 3:])\n",
    "            elif doc[i + 1].pos_ == \"PREP\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[11].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                return getPattern(doc[i + 3:])\n",
    "            elif doc[i + 1].pos_ == \"NOUN\":\n",
    "                if doc[i + 2].pos_ == \"NOUN\":\n",
    "                    sentenceByPattern[6].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                    return getPattern(doc[i + 3:])\n",
    "                else:\n",
    "                    sentenceByPattern[1].append(doc[i].text + \" \" + doc[i + 1].text)\n",
    "                    return getPattern(doc[i + 2:])\n",
    "        elif doc[i].pos_ == \"VERB\":\n",
    "            if doc[i + 1].pos_ == \"PRON\":\n",
    "                if doc[i + 2].pos_ == \"ADJ\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "                    sentenceByPattern[17].append(doc[i].text + \" \"+ doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                    return getPattern(doc[i + 4:])\n",
    "                if doc[i + 2].pos_ == \"NOUN\":\n",
    "                    sentenceByPattern[7].append(doc[i].text +\" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                    return getPattern(doc[i + 3:])\n",
    "            elif doc[i + 1].pos_ == \"PREP\" and doc[i + 2].pos_ == \"ADJ\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[16].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                return getPattern(doc[i + 4:])\n",
    "            elif doc[i + 1].pos_ == \"NOUN\" :\n",
    "                if doc[i + 2].pos_ == \"PREP\" and doc[i + 3].pos_ == \"NOUN\":\n",
    "                    sentenceByPattern[13].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                    return getPattern(doc[i + 4:])\n",
    "                elif doc[i + 2].pos_ == \"NOUN\":\n",
    "                    sentenceByPattern[8].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                    return getPattern(doc[i + 3:])\n",
    "                else:\n",
    "                    sentenceByPattern[2].append(doc[i].text + \" \" + doc[i + 1].text)\n",
    "                    return getPattern(doc[i + 2:])\n",
    "            elif doc[i + 1].pos_ == \"DET\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[12].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                return getPattern(doc[i + 3:])\n",
    "            elif doc[i + 1].pos_ == \"ADJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[9].append(doc[i].text + \" \" + doc[i + 1].text + \" \" +doc[i + 2].text)\n",
    "                return getPattern(doc[i + 3:])\n",
    "        elif doc[i].pos_ == \"ADJ\" :\n",
    "            if doc[i + 1].pos_ == \"CONJ\" and doc[i + 2].pos_ == \"ADJ\":\n",
    "                sentenceByPattern[15].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                return getPattern(doc[i + 3:])\n",
    "            elif doc[i + 1].pos_ == \"NOUN\":\n",
    "                if doc[i + 2].pos_ == \"NOUN\":\n",
    "                    if doc[i + 3].pos_ == \"NOUN\":\n",
    "                        sentenceByPattern[14].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text + \" \" + doc[i + 3].text)\n",
    "                        return getPattern(doc[i + 4:])\n",
    "                    else:\n",
    "                        sentenceByPattern[5].append(doc[i].text + \" \" + doc[i + 1].text + \" \" + doc[i + 2].text)\n",
    "                        return getPattern(doc[i + 3:])\n",
    "                else:\n",
    "                    sentenceByPattern[3].append(doc[i].text + \" \" + doc[i + 1].text)\n",
    "                    return getPattern(doc[i + 2:])\n",
    "            elif doc[i + 1].pos_ == \"ADJ\" and doc[i + 2].pos_ == \"NOUN\":\n",
    "                sentenceByPattern[10].append(doc[i].text + \" \" + doc[i + 1].text +\" \" + doc[i + 2].text)\n",
    "                return getPattern(doc[i + 3:])\n",
    "        else:\n",
    "            return getPattern(doc[i+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getPattern(nlp(united_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dizaine NOUN ROOT\n",
      "jours NOUN punct\n",
      "sorties ADJ ROOT\n",
      "pied ADJ ROOT\n",
      "acc√®s NOUN ROOT\n",
      "donn√©es ADJ amod\n",
      "synth√®se NOUN amod\n",
      "propos√© VERB acl\n",
      "application NOUN nmod\n",
      "allure ADJ amod\n",
      "moyenne NOUN advmod\n",
      "plus ADV advmod\n",
      "rythme NOUN obl:arg\n",
      "moyen ADJ amod\n",
      "plus ADV advmod\n",
      "de ADP case\n",
      "vitesse NOUN nmod\n",
      "moyenne ADJ amod\n",
      "explication NOUN amod\n",
      "solution NOUN amod\n",
      "bonne ADJ amod\n",
      "application NOUN obj\n",
      "ou CCONJ cc\n",
      "manque ADJ conj\n",
      "mode NOUN nsubj\n",
      "fractionn√© VERB ROOT\n",
      "ergonomique ADJ amod\n",
      "tr√®s ADV advmod\n",
      "intuitive ADJ advcl\n"
     ]
    }
   ],
   "source": [
    "for token in nlp(united_string):\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['dizaine jours'],\n",
       " ['propos√© application'],\n",
       " ['donn√©es synth√®se', 'allure moyenne', 'bonne application', 'manque mode'],\n",
       " [],\n",
       " ['moyenne explication solution'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['sorties pied acc√®s'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceByPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I'd say that \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Facebook is evil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\n",
       "</mark>\n",
       ".</div>\n",
       "\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">‚Äì \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Facebook is pretty cool\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MATCH</span>\n",
       "</mark>\n",
       ", right?</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    # Append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"MATCH\",\n",
    "    }]\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "           {\"POS\": \"ADJ\"}]\n",
    "matcher.add(\"FacebookIs\", [pattern], on_match=collect_sents)  # add pattern\n",
    "doc = nlp(\"I'd say that Facebook is evil. ‚Äì Facebook is pretty cool, right?\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Serve visualization of sentences containing match with displaCy\n",
    "# set manual=True to make displaCy render straight from a dictionary\n",
    "# (if you're not running the code within a Jupyer environment, you can\n",
    "# use displacy.serve instead)\n",
    "displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\"} {\"POS\": \"ADJ\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16743743820210141046 4 0 3 Song and artist\n",
      "3049302120552557844 18 0 4 Song and artist album\n",
      "5533571732986600803 1 2 4 artist album\n",
      "6675425533856447351 17 6 10 Create your greatest album\n",
      "602994839685422785 3 8 10 greatest album\n",
      "4206877449096176963 16 11 15 Choose from popular versions\n",
      "602994839685422785 3 13 15 popular versions\n",
      "4206877449096176963 16 15 19 Choose from popular versions\n",
      "602994839685422785 3 17 19 popular versions\n",
      "602994839685422785 3 19 21 Fast system\n",
      "2090661578966068036 5 19 22 Fast system virus\n",
      "5533571732986600803 1 20 22 system virus\n",
      "9798277639574861054 14 19 23 Fast system virus scanner\n",
      "10999827425508017904 6 20 23 system virus scanner\n",
      "5533571732986600803 1 21 23 virus scanner\n",
      "15180167692696242062 2 23 25 Use depth\n",
      "6349566914108460152 13 23 27 Use depth of field\n",
      "2988402087957123519 11 24 27 depth of field\n",
      "1124146173557384544 12 27 30 Share an image\n",
      "6572986864102252890 10 33 36 Super bright flashlight\n",
      "602994839685422785 3 34 36 bright flashlight\n",
      "1819085394523955522 9 36 39 Perform intuitive gestures\n",
      "602994839685422785 3 37 39 intuitive gestures\n",
      "15180167692696242062 2 39 41 Enjoy group\n",
      "5117079446564601502 8 39 42 Enjoy group conversations\n",
      "5533571732986600803 1 40 42 group conversations\n",
      "2462676316711722248 7 42 45 Share your thoughts\n",
      "5533571732986600803 1 44 46 thoughts email\n",
      "15180167692696242062 2 46 48 chat history\n",
      "602994839685422785 3 48 50 Live traffic\n",
      "2090661578966068036 5 48 51 Live traffic conditions\n",
      "5533571732986600803 1 49 51 traffic conditions\n",
      "1819085394523955522 9 53 56 tablet Precise location\n",
      "602994839685422785 3 54 56 Precise location\n",
      "15180167692696242062 2 56 58 Send message\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern1 = [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern2 = [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern3 = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern4 = [{\"POS\": \"NOUN\"}, {\"POS\": \"CCONJ\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern5 = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern6 = [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern7 = [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern8 = [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern9 = [{\"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern10 = [{\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern11 = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADP\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern12 = [{\"POS\": \"VERB\"}, {\"POS\": \"DET\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern13 = [{\"POS\": \"VERB\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"ADP\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern14 = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern15 = [{\"POS\": \"ADJ\"}, {\"POS\": \"CONJ\"}, {\"POS\": \"ADJ\"}]\n",
    "pattern16 = [{\"POS\": \"VERB\"}, {\"POS\": \"ADP\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern17 = [{\"POS\": \"VERB\"}, {\"POS\": \"PRON\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}]\n",
    "pattern18 = [{\"POS\": \"NOUN\"}, {\"POS\": \"CCONJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "matcher.add(\"18\", [pattern18])\n",
    "matcher.add(\"17\", [pattern17])\n",
    "matcher.add(\"16\", [pattern16])\n",
    "matcher.add(\"15\", [pattern15])\n",
    "matcher.add(\"14\", [pattern14])\n",
    "matcher.add(\"13\", [pattern13])\n",
    "matcher.add(\"12\", [pattern12])\n",
    "matcher.add(\"11\", [pattern11])\n",
    "matcher.add(\"10\", [pattern10])\n",
    "matcher.add(\"9\", [pattern9])\n",
    "matcher.add(\"8\", [pattern8])\n",
    "matcher.add(\"7\", [pattern7])\n",
    "matcher.add(\"6\", [pattern6])\n",
    "matcher.add(\"5\", [pattern5])\n",
    "matcher.add(\"4\", [pattern4])\n",
    "matcher.add(\"3\", [pattern3])\n",
    "matcher.add(\"2\", [pattern2])\n",
    "matcher.add(\"1\", [pattern1])\n",
    "\n",
    "\n",
    "doc = nlp(\"Song and artist album,  Create your greatest album, Choose from popular versions Choose from popular versions Fast system virus scanner Use depth of field Share an image Highlight with colors Super bright flashlight Perform intuitive gestures Enjoy group conversations Share your thoughts email chat history Live traffic conditions Phone or tablet Precise location Send message Group conversation, nice and warm, hot with orange\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)\n",
    "\n",
    "#Remove duplicate and synonyms, noise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
