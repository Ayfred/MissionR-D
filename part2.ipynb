{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TÃ©lÃ©chargement des packages Python/spaCy\n",
    "#!python -m spacy download fr_core_news_sm\n",
    "import time\n",
    "start = time.time()\n",
    "#Importation des packages Python/spaCy\n",
    "import spacy\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                dizaine   jours      sorties     pied      n     accÃ¨s   donnÃ©es   synthÃ¨se   proposÃ©     application      d     allure   moyenne      plus   rythme   moyen      plus   de   vitesse   moyenne         explication      solution        bonne   application      manque   mode   fractionnÃ©        ergonomique   trÃ¨s   intuitive          \n"
     ]
    }
   ],
   "source": [
    "# open the CSV file using the csv module\n",
    "with open('GooglePlayComments.csv', newline='',  encoding='utf-8') as csvfile:\n",
    "    # create a csv reader object\n",
    "    reader = csv.reader(csvfile)\n",
    "    # create an empty list to store the strings\n",
    "    string_list = []\n",
    "    # loop through each row in the CSV file\n",
    "    for row in reader:\n",
    "        # add the string to the list\n",
    "        #print(row)\n",
    "        #if detect(row[0]) == 'fr':\n",
    "        string_list.append(row[0])\n",
    "\n",
    "#print(len(string_list))\n",
    "\n",
    "#print the list of strings\n",
    "#print(string_list)\n",
    "\n",
    "string_list_clean = []\n",
    "\n",
    "\n",
    "#Exemple de texte\n",
    "text = \"Selon WikipÃ©dia il l'appelle Laura ðŸ˜ŠðŸ˜ŠðŸ˜Š(hezignborzingiozrnoivznioznvionzo), La ponctuation est gÃ©nial lâ€™ensemble des signes qui, dans lâ€™Ã©crit, marquent les divisions et les liaisons des phrases et des membres 'de phrase'. Vous pouvez en apprendre plus sur \\\"la ponctuation\\\" en visitant ce site: https://www.larousse.fr/dictionnaires/francais/ponctuation/63717 parce que ce lien donne beaucoup d'informations intÃ©ressantes. ðŸ˜Š\"\n",
    "#text = \"FonctionnalitÃ©s, salut car beau temps aujourd'hui\"\n",
    "\n",
    "#Etapes du texte Preprocessing\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "listStopWords = [str(x) for x in nlp.Defaults.stop_words]\n",
    "\n",
    "#Tokenisation\n",
    "def tokenisation(text):\n",
    "    return nlp(text)\n",
    "\n",
    "#Affichage des tokens\n",
    "def affichage_tokens(doc):\n",
    "    for token in doc:\n",
    "        # Print the text and the predicted part-of-speech tag\n",
    "        print(token.text, token.pos_)\n",
    "\n",
    "#Removal of explanations\n",
    "def explanationsRemoval(doc):\n",
    "    sentence = doc.text\n",
    "    # ItÃ©rer Ã  travers chaque token dans le document\n",
    "    for token in doc:\n",
    "        # Si le token est \"parce que\", \"car\" ou \"puisque\", supprimer tous les tokens suivants dans la phrase\n",
    "        if token.text.lower() in [\"parce\", \"car\", \"puisque\"]:\n",
    "            sentence = sentence[:token.idx]\n",
    "            break\n",
    "    # Supprimer tout ce qui est entre parenthÃ¨ses\n",
    "    sentence = re.sub(r'\\([^()]*\\)', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "#Removal of quotes\n",
    "def quotesRemoval(doc):\n",
    "    filtered_text = \"\"\n",
    "    Punc_list = ['\"', \"'\"]\n",
    "    boolean_punct = False\n",
    "    for token in doc:\n",
    "        if not token.text in Punc_list:\n",
    "            if not boolean_punct:\n",
    "                filtered_text += token.text\n",
    "                filtered_text += \" \"\n",
    "        else :\n",
    "            if boolean_punct :\n",
    "                boolean_punct = False\n",
    "            else :\n",
    "                boolean_punct = True\n",
    "    return filtered_text\n",
    "\n",
    "#Removal of URLs\n",
    "def urlRemoval(text):\n",
    "    return re.sub('http[s]?://\\S+', '', text)\n",
    "\n",
    "#Removal of contact information\n",
    "def contactRemoval(text):\n",
    "    # Regular expressions for phone numbers and email addresses\n",
    "    phone_regex = r\"(?<!\\d)(?:\\d[ -/\\\\_\\d]*){10}(?!\\d)\"\n",
    "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"  # matches john.doe@example.com\n",
    "\n",
    "    # Find phone numbers and email addresses in the sentence\n",
    "    text = re.sub(phone_regex,\"\", text)\n",
    "    text = re.sub(email_regex,\"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "#Removal of symbols\n",
    "def symbolsRemoval(text):\n",
    "    return re.sub(r'[^\\w]', ' ', text)\n",
    "\n",
    "#Removal of stop words\n",
    "def stopWordsRemoval(text):\n",
    "    splitext = text.split(\" \")\n",
    "    for word in splitext:\n",
    "        if word in listStopWords:\n",
    "            splitext.remove(word)\n",
    "    return ' '.join(splitext)\n",
    "\n",
    "def pronounAndDetRemoval(doc):\n",
    "    #texte = doc.text\n",
    "    sentence = \"\"\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"PRON\" or token.pos_ == \"DET\"):\n",
    "            continue\n",
    "        else:\n",
    "            sentence += token.text_with_ws\n",
    "            sentence += \" \"\n",
    "    return sentence\n",
    "\n",
    "def propNounRemoval(doc):\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if not (token.ent_type_ == \"ORG\" or token.ent_type_ == \"PERSON\" or token.text.istitle()):\n",
    "            result.append(token.text)\n",
    "    return \" \".join(result)\n",
    "\n",
    "def united(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    sentence = doc.text\n",
    "    # ItÃ©rer Ã  travers chaque token dans le document\n",
    "    for token in doc:\n",
    "\n",
    "        #Remove explanations\n",
    "        # Si le token est \"parce que\", \"car\" ou \"puisque\", supprimer tous les tokens suivants dans la phrase\n",
    "        if token.text.lower() in [\"parce\", \"car\", \"puisque\"]:\n",
    "            sentence = sentence[:token.idx]\n",
    "            break\n",
    "    # Supprimer tout ce qui est entre parenthÃ¨ses\n",
    "    sentence = re.sub(r'\\([^()]*\\)', '', sentence)\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    #Filter\n",
    "    filtered_text = \"\"\n",
    "    Punc_list = ['\"', \"'\"]\n",
    "    boolean_punct = False\n",
    "    for token in doc:\n",
    "        if not token.text in Punc_list:\n",
    "            if not boolean_punct:\n",
    "                filtered_text += token.text\n",
    "                filtered_text += \" \"\n",
    "        else :\n",
    "            if boolean_punct :\n",
    "                boolean_punct = False\n",
    "            else :\n",
    "                boolean_punct = True\n",
    "\n",
    "    sentence = filtered_text\n",
    "    sentence = re.sub('http[s]?://\\S+', '', sentence)\n",
    "\n",
    "    # Regular expressions for phone numbers and email addresses\n",
    "    phone_regex = r\"(?<!\\d)(?:\\d[ -/\\\\_\\d]*){10}(?!\\d)\"\n",
    "    email_regex = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"  # matches john.doe@example.com\n",
    "\n",
    "    # Find phone numbers and email addresses in the sentence\n",
    "    sentence = re.sub(phone_regex,\"\", sentence)\n",
    "    sentence = re.sub(email_regex,\"\", sentence)\n",
    "    sentence = re.sub(r'[^\\w]', ' ', sentence)\n",
    "    splitext = sentence.split(\" \")\n",
    "    for word in splitext:\n",
    "        if word in listStopWords:\n",
    "            splitext.remove(word)\n",
    "    sentence= ' '.join(splitext)\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    #texte = doc.text\n",
    "    text = \"\"\n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"PRON\" or token.pos_ == \"DET\"):\n",
    "            continue\n",
    "        else:\n",
    "            text += token.text_with_ws\n",
    "            text += \" \"\n",
    "    sentence = text\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if not (token.ent_type_ == \"ORG\" or token.ent_type_ == \"PERSON\" or token.text.istitle()):\n",
    "            result.append(token.text)\n",
    "    sentence =  \" \".join(result)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "\"\"\"\n",
    "for string in string_list:\n",
    "    united_string = united(string)\n",
    "    string_list_clean.append(united_string)\n",
    "print(string_list_clean)\n",
    "#run\n",
    "#print(united(text))\"\"\"\n",
    "united_string = united(string_list[0])\n",
    "print(united_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy_langdetect import LanguageDetector\n",
    "\n",
    "def get_lang_detector(nlp, name):\n",
    "    return LanguageDetector()\n",
    "\n",
    "#Language.factory(\"language_detector\", func=get_lang_detector)\n",
    "#nlp.add_pipe('language_detector', last=True)\n",
    "#text = 'ã¯ã„ã‚ãƒãŒã¨ã†.'\n",
    "#doc = nlp(text)\n",
    "#print(doc._.language)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                dizaine   jours      sorties     pied      n     accÃ¨s   donnÃ©es   synthÃ¨se   proposÃ©     application      d     allure   moyenne      plus   rythme   moyen      plus   de   vitesse   moyenne         explication      solution        bonne   application      manque   mode   fractionnÃ©        ergonomique   trÃ¨s   intuitive          '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "united_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                \n",
      "dizaine\n",
      "  \n",
      "jours\n",
      "     \n",
      "sorties\n",
      "    \n",
      "pied\n",
      "     \n",
      "n\n",
      "    \n",
      "accÃ¨s\n",
      "  \n",
      "donnÃ©es\n",
      "  \n",
      "synthÃ¨se\n",
      "  \n",
      "proposÃ©\n",
      "    \n",
      "application\n",
      "     \n",
      "d\n",
      "    \n",
      "allure\n",
      "  \n",
      "moyenne\n",
      "     \n",
      "plus\n",
      "  \n",
      "rythme\n",
      "  \n",
      "moyen\n",
      "     \n",
      "plus\n",
      "  \n",
      "de\n",
      "  \n",
      "vitesse\n",
      "  \n",
      "moyenne\n",
      "        \n",
      "explication\n",
      "     \n",
      "solution\n",
      "       \n",
      "bonne\n",
      "  \n",
      "application\n",
      "     \n",
      "manque\n",
      "  \n",
      "mode\n",
      "  \n",
      "fractionnÃ©\n",
      "       \n",
      "ergonomique\n",
      "  \n",
      "trÃ¨s\n",
      "  \n",
      "intuitive\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "doc_2 = nlp(united_string)\n",
    "for token in doc_2:\n",
    "    print(token.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
